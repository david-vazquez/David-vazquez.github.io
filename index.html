<!DOCTYPE html>
<html lang="en">
  <head>
    <meta content="text/html; charset=UTF-8" http-equiv="content-type">
    <title>David Vazquez | Ph.D. in Computer Science</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <meta name="description" content="">
    <meta name="keywords" content="">
    <meta name="author" content="">
    <link rel="shortcut icon" href="images/favicon.ico">
    <!-- CSS | STYLE -->
    <link rel="stylesheet" type="text/css" href="css/bootstrap.min.css">
    <link rel="stylesheet" type="text/css" href="css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="css/linecons.css">
    <link rel="stylesheet" type="text/css" href="css/normalize.css">
    <link rel="stylesheet" type="text/css" href="css/colors/green.css">
    <link rel="stylesheet" type="text/css" href="css/style.css">
    <!-- CSS | Google Fonts -->
    <link href="http://fonts.googleapis.com/css?family=Montserrat:400" rel="stylesheet"
      type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Raleway:200,400,300,500,600"
      rel="stylesheet" type="text/css">
    <!-- <script type="text/javascript">$(document).ready(function(){mainF('author=David%20Vazquez');});</script>-->
    <!-- <script type="text/javascript">mainF('author=David%20Vazquez');</script> -->
    <script type="text/javascript" src="./jquery.js"></script>
    <script type="text/javascript" src="./pubshow.js"></script>

    <noscript>
        <style>
        @media screen and (max-width: 755px) {
            .hs-content-scroller {
                overflow: visible;
            }
        }
        </style>
    </noscript>
  </head>
  <body> <!-- Page preloader -->
    <div id="page-loader"> <canvas id="demo-canvas"></canvas> </div>
    <!-- container -->
    <div id="hs-container" class="hs-container">
      <!-- Sidebar-->
      <div class="aside1"> <a class="contact-button"><i class="fa fa-paper-plane"></i></a>
        <a class="download-button"><i class="fa fa-cloud-download"></i></a>
        <div class="aside-content"><span class="part1">DAVID VÁZQUEZ</span><span class="part2">Academic Personal Site</span> </div>
      </div>
      <aside class="hs-menu" id="hs-menu">
        <!-- <canvas id="demo-canvas"></canvas> -->
        <!-- Profil Image-->
        <div class="hs-headline"> <a id="my-link" href="#my-panel"><i class="fa fa-bars"></i></a>
          <a href="#" class="download"><i class="fa fa-cloud-download"></i></a>
          <div class="img-wrap"> <img src="images/portrait.png" alt="" width="150"> </div>
          <div class="profile_info">
            <h1>David Vázquez</h1>
            <h4>PhD Computer Science</h4>
            <h6><span class="fa fa-location-arrow"></span>&nbsp;&nbsp;&nbsp;Montreal, Canada</h6>
          </div>
          <div style="clear:both"></div>
        </div>
        <div class="separator-aside"></div>
        <!-- End Profil Image-->
        <!-- menu -->
        <nav> <a href="#section1"><span class="menu_name">ABOUT</span><span class="fa fa-home"></span>
          </a> <a href="#section2"><span class="menu_name">RESUME</span><span class="fa fa-newspaper-o"></span>
          </a> <a href="#section3"><span class="menu_name">PUBLICATIONS</span><span
              class="fa fa-pencil"></span> </a> <a href="#section4"><span class="menu_name">RESEARCH</span><span
              class="fa fa-flask"></span> </a> <a href="#section5"><span class="menu_name">TEACHING</span><span
              class="fa fa-book"></span> </a> <a href="#section6"><span class="menu_name">SKILLS</span><span
              class="fa fa-diamond"></span> </a> <a href="#section7"><span class="menu_name">WORKS</span><span
              class="fa fa-archive"></span> </a> <a href="#section8"><span class="menu_name">CONTACT</span><span
              class="fa fa-paper-plane"></span> </a> </nav>
        <!-- end menu-->
        <!-- social icons -->
        <div class="aside-footer">
        <!--<a href="#"><i class="fa fa-facebook"></i></a> -->
          <a href="https://twitter.com/dvazquezcv?lang=en"><i class="fa fa-twitter"></i></a>
          <a href="https://www.linkedin.com/in/dvazquezcvc/"><i class="fa fa-linkedin"></i></a>
        <!-- <a href="#"><i class="fa fa fa-dribbble"></i></a> -->
          <a href="https://github.com/david-vazquez"><i class="fa fa fa-github"></i></a>
        </div>
        <!-- end social icons --> </aside>
      <!-- End sidebar -->
      <!-- Go To Top Button -->
      <!-- End Go To Top Button -->
      <!-- hs-content-scroller -->
      <div class="hs-content-scroller">
        <!-- Header -->
        <div id="header_container">
          <div id="header">
            <div><a class="home"><i class="fa fa-home"></i></a> </div>
            <div><a href="" class="previous-page arrow"><i class="fa fa-angle-left"></i></a>
            </div>
            <div><a href="" class="next-page arrow"><i class="fa fa-angle-right"></i></a>
            </div>
            <!-- News scroll -->
            <div class="news-scroll"> <span><i class="fa fa-line-chart"></i>RECENT
                ACTIVITY : </span>
              <ul id="marquee" class="marquee">
                <li>08-04-2016 - Best poster award: My student Victor Campmany gets the best poster award at NVIDIA GTC 2016 conference.</li>
		        <li>30.03.2016 - Internship: I start my internship at MILA laboratory at Montreal University under the supervision of Aaron Courville</li>
		        <li>15.12.2015 - Marie Curie Grant: I have been awarded with the Marie Curie Grant under the program TecnioSping.</li>
		        <li>15.02.2016 - Accepted paper: Our paper Hierarchical online domain adaptation of deformable part-based models has been acepted at IJCV.</li>
		        <li>15.03.2016 - Accepted paper: Two papers has been accepted at the ICCS conference. One as oral.</li>
                <li>15.03.2016 - Best poster award: Our paper on GPU-based pedestrian detection for autonomous driving has been selected as one of the 5 best posters of the GTC NVIDIA conference 2016.</li>
              </ul>
            </div>
            <!-- End News scroll --> </div>
        </div>
        <!-- End Header -->
        <!-- hs-content-wrapper -->
        <div class="hs-content-wrapper">
          <!-- About section -->
          <article class="hs-content about-section" id="section1"> <span class="sec-icon fa fa-home"></span>
            <div class="hs-inner"> <span class="before-title">.01</span>
              <h2>ABOUT</h2>
              <span class="content-title">PERSONAL DETAILS</span>
              <div class="aboutInfo-contanier">
                <div class="about-card">
                  <div class="face2 card-face">
                    <div id="cd-google-map">
                      <div id="google-container"></div>
                      <div id="cd-zoom-in"></div>
                      <div id="cd-zoom-out"></div>
                      <address>Element AI, Montreal, Canada</address>
                      <div class="back-cover" data-card-back="data-card-back"><i
                          class="fa fa-long-arrow-left"></i> </div>
                    </div>
                  </div>
                  <div class="face1 card-face">
                    <div class="about-cover card-face"> <a class="map-location"
                        data-card-front="data-card-front"><img src="images/map-icon.png"
                          alt=""> </a>
                      <div class="about-details">
                        <div><span class="fa fa-inbox"></span><span class="detail">dvazquez@elementai.com</span>
                        </div>
                        <div><span class="fa fa-phone"></span><span class="detail">+1 438 395 6312</span> </div>
                      </div>
                      <div class="cover-content-wrapper"> <span class="about-description">Hello.
                          I am a <span class="rw-words">
				                    <span><strong>Researcher</strong></span>
                            	    <span><strong>Dreamer</strong></span>
                                    <span><strong>Programmer</strong></span>
                                    <span><strong>Leader</strong></span>
				                 <span>
                          <strong>Autonomous Driving Engineer</strong></span>
                          </span> <br>
                          I am passionate about Deep Learning and autonomous vehicles. Welcome to my Personal and Academic profile</span> <span
                          class="status"> <span class="fa fa-circle"></span> <span
                            class="text">Available as <strong>researcher</strong></span>
                        </span> </div>
                    </div>
                  </div>
                </div>
                <div class="more-details">
                  <div class="tabbable tabs-vertical tabs-left">
                    <ul id="myTab" class="nav nav-tabs">
                      <li class="active"> <a href="#bio" data-toggle="tab">Bio</a>
                      </li>
                      <li> <a href="#hobbies" data-toggle="tab">Hobbies</a> </li>
                      <li> <a href="#facts" data-toggle="tab">Facts</a> </li>
                    </ul>
                    <div id="myTabContent" class="tab-content">
                      <div class="tab-pane fade in active" id="bio">
                        <h3>BIO</h3>
                        <h4>ABOUT ME</h4>
                        <p>Dr. David Vázquez is a Research Scientist at Element AI. Previously he was a postdoctoral researcher at Computer Vision Center of Barcelona (CVC) and Montreal Institut of Learning Algorithms (MILA) and Asistant Professor in the Department of Computer Science at the Autonomous University of Barcelona (UAB). He received his Ph.D. in Computer Vision (2013), M.Sc. in CV and AI (2009) and B.Sc. in Computer Science (2008) from the UAB. Previously he received the B.Sc. in Software Engineering from the UDC (2006). He has done internships at Daimler AG, UAM and URJC. He is expert in machine perception for autonomous vehicles. His research interests include deep learning, computer vision, robotics and autonomous driving.</p>
                        <p>He is a recipient of four awards for his Ph.D. Thesis by a Spahish chapter of the Intelligent Transportation Systems Society (ITSS); the Spanish Chapter of the International Association of Pattern Recognition (IAPR); the UAB; and the Centres de Recerca de Catalunya (CERCA); three best paper awards (GTC2016, NIPs-Workshop2011, ICMI2011) and two challenges (CVPR-Challenge2013&2014). David has also participated in industrial projects with companies such as IDIADA Applus+, Samsung and Audi.</p>
                        <p>David, has been organizer of international workshops in main conferences (i.e., TASK-CV, CVVT, VARVAI) chair at conferences (i.e., IBPRIA), an Editor of the IET Computer Vision journal (IET-CV)and has served as Program Committe of multiple machine learning and vision conferences and Journals (i.e., NIPS, CVPR, ECCV, ICCV, BMVC).</p>
                      </div>
                      <div class="tab-pane fade" id="hobbies">
                        <h3>HOBBIES</h3>
                        <h4>INTERESTS</h4>
                        <div class="hobbie-wrapper row">
                          <div class="hobbie-icon col-md-3"><i class="li_t-shirt"></i>
                          </div>
                          <div class="hobbie-description col-md-9">
                            <p>I'm passionate about conditioning training group classes such Bodypump, bodycombat, and any aerobics class.</p>
                          </div>
                          <div style="clear:both;"></div>
                        </div>
                        <div class="hobbie-wrapper row">
                          <div class="hobbie-icon col-md-3"><i class="li_food"></i>
                          </div>
                          <div class="hobbie-description col-md-9">
                            <p>I like experimenting with new vegetarian receips. I'm a Thermomix Master..</p>
                          </div>
                        </div>
                        <div class="hobbie-wrapper row">
                          <div class="hobbie-icon col-md-3"><i class="li_lab"></i>
                          </div>
                          <div class="hobbie-description col-md-9">
                            <p>I like programming for different platforms such arduino, raspberry py or Jetson.</p>
                          </div>
                        </div>
                        <div class="hobbie-wrapper row">
                          <div class="hobbie-icon col-md-3"><i class="li_tv"></i>
                          </div>
                          <div class="hobbie-description col-md-9">
                            <p>Watching funny tv series allow me to disconnect and get asleep.</p>
                          </div>
                        </div>
                        <div style="clear:both;"></div>
                      </div>
                      <div class="tab-pane fade" id="facts">
                        <h3>FACTS</h3>
                        <h4>NUMBERS ABOUT ME</h4>
                        <div class="facts-wrapper col-md-6">
                          <div class="facts-icon"><i class=" li_note"></i> </div>
                          <div class="facts-number">50</div>
                          <div class="facts-description">PAPERS PUBLISHED</div>
                        </div>
                        <div class="facts-wrapper col-md-6">
                          <div class="facts-icon"><i class="li_bulb"></i> </div>
                          <div class="facts-number">9</div>
                          <div class="facts-description">PROJECTS COMPLETED</div>
                        </div>
                        <div class="facts-wrapper col-md-6">
                          <div class="facts-icon"><i class="li_t-shirt"></i> </div>
                          <div class="facts-number">7</div>
                          <div class="facts-description">WORKSHOPS ORGANIZED</div>
                        </div>
                        <div class="facts-wrapper col-md-6">
                          <div class="facts-icon"><i class="li_diamond"></i> </div>
                          <div class="facts-number">9</div>
                          <div class="facts-description">AWARDS</div>
                        </div>
                        <div class="facts-wrapper col-md-6">
                          <div class="facts-icon"><i class="li_clock"></i> </div>
                          <div class="facts-number">7300</div>
                          <div class="facts-description">HOURS OF CODING</div>
                        </div>
                        <div class="facts-wrapper col-md-6">
                          <div class="facts-icon"><i class="li_display"></i> </div>
                          <div class="facts-number">2M</div>
                          <div class="facts-description">LINES OF CODE</div>
                        </div>
                        <div style="clear:both;"></div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            <br>
          </article>
          <!-- End About Section -->
          <!-- Resume Section -->
          <article class="hs-content resume-section" id="section2"> <span class="sec-icon fa fa-newspaper-o"></span>
            <div class="hs-inner"> <span class="before-title">.02</span>
              <h2>RESUME</h2>
              <!-- Resume Wrapper -->
              <div class="resume-wrapper">
                <ul class="resume">
                  <!-- Resume timeline -->
                  <li class="time-label"> <span class="content-title">EDUCATION</span>
                  </li>
                  <li>
                    <div class="resume-tag"> <span class="fa fa-graduation-cap"></span>
                      <div class="resume-date"> <span>2013</span>
                        <div class="separator"></div>
                        <span>2010</span> </div>
                    </div>
                    <div class="timeline-item"> <span class="timeline-location"><iclass="fa fa-map-marker"></i>Barcelona</span>
                      <h3 class="timeline-header">COMPUTER VISION - PHD</h3>
                      <div class="timeline-body">
                        <h4>AUTONOMOUS UNIVERSITY OF BARCELONA</h4>
                        <span>
			              <b>Thesis:</b> Virtual and Real World Adaptation for Pedestrian Detection<br>
                          <b>Grant:</b> PIF Autonomous University grant.<br>
                        </span>
                      </div>
                    </div>
                  </li>
                  <li>
                    <div class="resume-tag"> <span class="fa fa-graduation-cap"></span>
                      <div class="resume-date"> <span>2009</span>
                        <div class="separator"></div>
                        <span>2008</span> </div>
                    </div>
                    <div class="timeline-item"> <span class="timeline-location"><i
                          class="fa fa-map-marker"></i>Barcelona</span>
                      <h3 class="timeline-header">COMPUTER VISION & ARTIFICIAL INTELLIGENCE - MASTER</h3>
                      <div class="timeline-body">
                        <h4>AUTONOMOUS UNIVERSITY OF BARCELONA</h4>
                        <span>
                            <b>Dissertation:</b> The effect of the distance in Pedestrian Detection.<br>
                            <b>Grant:</b> PIF Autonomous University grant.<br>
                        </span>
                      </div>
                    </div>
                  </li>
                  <li>
                    <div class="resume-tag"> <span class="fa fa-graduation-cap"></span>
                      <div class="resume-date"> <span>2008</span>
                        <div class="separator"></div>
                        <span>2006</span> </div>
                    </div>
                    <div class="timeline-item"> <span class="timeline-location"><i
                          class="fa fa-map-marker"></i>Barcelona</span>
                      <h3 class="timeline-header">COMPUTER SCIENCE - DEGREE</h3>
                      <div class="timeline-body">
                        <h4>AUTONOMOUS UNIVERSITY OF BARCELONA</h4>
                        <span>
                            <b>Dissertation:</b> Intrusion Detection in Intelligent Surveillance Systems.<br>
                            <b>Grant:</b> SICUE-Seneca mobity grant. Intern at UAB & URJC.<br>
                        </span>
                      </div>
                    </div>
                  </li>
                  <li>
                    <div class="resume-tag"> <span class="fa fa-graduation-cap"></span>
                      <div class="resume-date"> <span>2008</span>
                        <div class="separator"></div>
                        <span>2006</span> </div>
                    </div>
                    <div class="timeline-item"> <span class="timeline-location"><i
                          class="fa fa-map-marker"></i>A Coruña</span>
                      <h3 class="timeline-header">SOFTWARE ENGINEERING - DEGREE</h3>
                      <div class="timeline-body">
                        <h4>UNIVERSITY OF A CORUÑA</h4>
                        <span><b>Dissertation:</b> Face Recognition in Barajas Airport.</span> </div>
                    </div>
                  </li>

                  <!--- ACADEMIC AND PROFESSIONAL POSITIONS --->
                  <!------------------------------------------->
                  <li class="time-label"> <span class="content-title">ACADEMIC AND PROFESSIONAL POSITIONS</span> </li>
                  <li>
                    <div class="resume-tag"> <span class="fa fa-university"></span>
                      <div class="resume-date"> <span>Now</span>
                        <div class="separator"></div>
                        <span>2018</span> </div>
                    </div>
                    <div class="timeline-item"> <span class="timeline-location"><i
                          class="fa fa-map-marker"></i>Montreal</span>
                      <h3 class="timeline-header">RESEARCH SCIENTIST</h3>
                      <div class="timeline-body">
                        <h4>ELEMENT AI</h4>
                        <span><b>Group:</b> Computer Vision - Fundamental Research</span>
                      </div>
                    </div>
                  </li>
                  <li>
                    <div class="resume-tag"> <span class="fa fa-university"></span>
                      <div class="resume-date"> <span>2018</span>
                        <div class="separator"></div>
                        <span>2016</span> </div>
                    </div>
                    <div class="timeline-item"> <span class="timeline-location"><i
                          class="fa fa-map-marker"></i>Montreal - Barcelona</span>
                      <h3 class="timeline-header">POSTDOCTORAL RESEARCHER</h3>
                      <div class="timeline-body">
                        <h4>MONTREAL INSTITUTE OF LEARNING ALGORITHMS & COMPUTER VISION CENTER BARCELONA</h4>
                        <span><b>Grant:</b> Marie Curie European Grant (Tecniospring)</span> </div>
                    </div>
                  </li>
                  <li>
                    <div class="resume-tag"> <span class="fa fa-university"></span>
                      <div class="resume-date"> <span>2017</span>
                        <div class="separator"></div>
                        <span>2008</span> </div>
                    </div>
                    <div class="timeline-item"> <span class="timeline-location"><i
                          class="fa fa-map-marker"></i>Barcelona</span>
                      <h3 class="timeline-header">ASSISTANT PROFESSOR</h3>
                      <div class="timeline-body">
                        <h4>AUTONOMOUS UNIVERSITY OF BARCELONA</h4>
                        <span><b>Subjects:</b>Machine learning (Master), Visual Recognition (MAster), Artificial Intelligence I & II (BSc), Software Engineering I & II (BSc) </span> </div>
                    </div>
                  </li>
                  <li>
                    <div class="resume-tag"> <span class="fa fa-university"></span>
                      <div class="resume-date"> <span>2016</span>
                        <div class="separator"></div>
                        <span>2013</span> </div>
                    </div>
                    <div class="timeline-item"> <span class="timeline-location"><i
                          class="fa fa-map-marker"></i>Barcelona</span>
                      <h3 class="timeline-header">POSTDOCTORAL RESEARCHER</h3>
                      <div class="timeline-body">
                        <h4>COMPUTER VISION CENTER BARCELONA</h4>
                        <span><b>Grant:</b> Juan de la Cierva Spanish Grant</span> </div>
                    </div>
                  </li>
                  <li>
                    <div class="resume-tag"> <span class="fa fa-university"></span>
                      <div class="resume-date"> <span>2013</span>
                        <div class="separator"></div>
                        <span>2009</span> </div>
                    </div>
                    <div class="timeline-item"> <span class="timeline-location"><i
                          class="fa fa-map-marker"></i>Barcelona</span>
                      <h3 class="timeline-header">GRADUATE STUDENT RESEARCHER</h3>
                      <div class="timeline-body">
                        <h4>COMPUTER VISION CENTER BARCELONA</h4>
                        <span><b>Grant:</b> PIF Autonomous University grant</span> </div>
                    </div>
                  </li>
                  <li>
                    <div class="resume-tag"> <span class="fa fa-university"></span>
                      <div class="resume-date"> <span>2009</span>
                        <div class="separator"></div>
                        <span>2007</span> </div>
                    </div>
                    <div class="timeline-item"> <span class="timeline-location"><i
                          class="fa fa-map-marker"></i>Barcelona</span>
                      <h3 class="timeline-header">LAB ASSISTANT</h3>
                      <div class="timeline-body">
                        <h4>COMPUTER VISION CENTER BARCELONA</h4>
                        <span><b>Grant:</b> Davantis Start-up Company intern</span> </div>
                    </div>
                  </li>

                  <li class="time-label"> <span class="content-title">HONORS AND AWARDS</span> </li>
                  <li>
                    <div class="resume-tag"> <span class="fa fa-star-o"></span>
                      <div class="resume-date"> <span>2016</span>
                        </div>
                    </div>
                    <div class="timeline-item"> <span class="timeline-location"><i
                          class="fa fa-map-marker"></i>Santa Clara</span>
                      <h3 class="timeline-header">BEST PAPER AWARD</h3>
                      <div class="timeline-body">
                        <h4>NVIDIA GTC 2016</h4>
                        <span><b>Article:</b> GPU-based pedestrian detection for autonomous driving</span> </div>
                    </div>
                  </li>

                  <li>
                    <div class="resume-tag"> <span class="fa fa-star-o"></span>
                      <div class="resume-date"> <span>2015</span>
                        </div>
                    </div>
                    <div class="timeline-item"> <span class="timeline-location"><i
                          class="fa fa-map-marker"></i>Madrid</span>
                      <h3 class="timeline-header">BEST PHD THESIS AWARD</h3>
                      <div class="timeline-body">
                        <h4>IEEE INTELLIGENT TANSPORTATION SYSTEMS SOCIETY SPANISH SECTION (ITSS)</h4>
                        <span></span> </div>
                    </div>
                  </li>

                  <li>
                    <div class="resume-tag"> <span class="fa fa-star-o"></span>
                      <div class="resume-date"> <span>2015</span>
                        </div>
                    </div>
                    <div class="timeline-item"> <span class="timeline-location"><i
                          class="fa fa-map-marker"></i>Santiago de Compostela</span>
                      <h3 class="timeline-header">BEST PHD THESIS AWARD</h3>
                      <div class="timeline-body">
                        <h4>SPANISH COMPUTER VISION ASSOCIATION (AERFAI)</h4>
                        <span></span> </div>
                    </div>
                  </li>

                  <li>
                    <div class="resume-tag"> <span class="fa fa-star-o"></span>
                      <div class="resume-date"> <span>2015</span>
                        </div>
                    </div>
                    <div class="timeline-item"> <span class="timeline-location"><i
                          class="fa fa-map-marker"></i>Barcelona</span>
                      <h3 class="timeline-header">BEST PHD THESIS AWARD</h3>
                      <div class="timeline-body">
                        <h4>AUTONOMOUS UNIVERSITY OF BARCELONA (UAB)</h4>
                        <span></span> </div>
                    </div>
                  </li>

                  <li>
                    <div class="resume-tag"> <span class="fa fa-star-o"></span>
                      <div class="resume-date"> <span>2014</span>
                        </div>
                    </div>
                    <div class="timeline-item"> <span class="timeline-location"><i
                          class="fa fa-map-marker"></i>Barcelona</span>
                      <h3 class="timeline-header">BEST PHD THESIS AWARD</h3>
                      <div class="timeline-body">
                        <h4>CERCA</h4>
                        <span></span> </div>
                    </div>
                  </li>

                  <li>
                    <div class="resume-tag"> <span class="fa fa-star-o"></span>
                      <div class="resume-date"> <span>2014</span>
                        <div class="separator"></div>
                        <span>2013</span>
                        </div>
                    </div>
                    <div class="timeline-item"> <span class="timeline-location"><i
                          class="fa fa-map-marker"></i>USA</span>
                      <h3 class="timeline-header">RMRC CHALLENGE</h3>
                      <div class="timeline-body">
                        <h4>ICCV 2013 & ECCV 2014</h4>
                        <span>
				            <b>RMRC Challenge (ECCV 2014):</b> Second position in Pedestrian Detection<br>
		                    <b>RMRC Challenge (ICCV 2013):</b> First position in Pedestrian Detection
			            </span>
                      </div>
                    </div>
                  </li>

                  <li>
                    <div class="resume-tag"> <span class="fa fa-star-o"></span>
                      <div class="resume-date"> <span>2011</span>
                        </div>
                    </div>
                    <div class="timeline-item"> <span class="timeline-location"><i
                          class="fa fa-map-marker"></i>Granada</span>
                      <h3 class="timeline-header">BEST PAPER AWARD</h3>
                      <div class="timeline-body">
                        <h4>NIPS DOMAIN ADAPTATION WORKSHOP: Theory and Application</h4>
                        <span><b>Article:</b> Cool world: domain adaptation of virtual and real worlds for human detection using active learning</span> </div>
                    </div>
                  </li>

                  <li>
                    <div class="resume-tag"> <span class="fa fa-star-o"></span>
                      <div class="resume-date"> <span>2011</span>
                        </div>
                    </div>
                    <div class="timeline-item"> <span class="timeline-location"><i
                          class="fa fa-map-marker"></i>Granada</span>
                      <h3 class="timeline-header">BEST PAPER AWARD</h3>
                      <div class="timeline-body">
                        <h4>ICMI DOCTORAL CONSORTIUM</h4>
                        <span><b>Article:</b> Virtual Worlds and Active Learning for Human Detection</span> </div>
                    </div>
                  </li>

                  <!-- End Resume timeline -->
                </ul>
              </div>
              <!-- End Resume Wrapper --> </div>
          </article>
          <!-- End Resume Section-->

          <!-- Publication Section -->
          <article class="hs-content publication-section" id="section3"> <span class="sec-icon fa fa-pencil"></span>
            <div class="hs-inner"> <span class="before-title">.03</span>
              <h2>PUBLICATIONS</h2>

              <!-- Filter/Sort Menu -->
              <span class="content-title">PUBLICATIONS LIST</span>
              <div class="row publication-form">
                <div class="col-md-6 publication-filter">
                  <div class="card-drop"> <a class="toggle"> <i class="icon-suitcase"></i>
                      <span class="label-active">ALL</span> </a>
                      <ul id="filter">
                        <li class="active"><a data-label="ALL" data-group="all">ALL</a> </li>
                        <li><a data-label="JOURNAL PAPERS" data-group="JOURNAL PAPERS">JOURNAL PAPERS</a> </li>
                        <li><a data-label="CONFERENCES" data-group="CONFERENCES">CONFERENCES</a> </li>
                        <li><a data-label="DEMONSTRATIONS" data-group="DEMONSTRATIONS">DEMONSTRATIONS</a> </li>
                        <li><a data-label="THESES" data-group="THESES">THESES</a> </li>
                        <li><a data-label="BOOK CHAPTERS" data-group="BOOK CHAPTERS">BOOK CHAPTERS</a> </li>
                        <li><a data-label="BOOK" data-group="BOOK">BOOK</a> </li>
                      </ul>
                  </div>
                </div>
                <div class="col-md-6 publication-sort">
                  <div class="sorting-button"> <span>SORTING BY DATE</span> <button class="desc"><i class="fa fa-sort-numeric-desc"></i> </button>
                    <button class="asc"><i class="fa fa-sort-numeric-asc"></i> </button>
                  </div>
                </div>
              </div>
              <!-- End Filter/Sort Menu -->

              <!-- publication wrapper -->
              <!-- <div id="mygrid"></div> -->
              <div id="mygrid2">

                  <div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2017-01-01"><div class="media"> <a href=".publication-detailHEV2017b" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_HEV2017b"> <img class="img-responsive" src="./Pubs/HEV2017b/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2017</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/HEV2016.pdf">GPU-ACCELERATED REAL-TIME STIXEL COMPUTATION</a></h3> <h4>IEEE WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION (<b>WACV</b>), 2017</h4> <span class="publication_description">The Stixel World is a medium-level, compact representation of road scenes that abstracts millions of disparity pixels into hundreds or thousands of stixels. The goal of this work is to implement and e...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_HEV2017b"></span> <span class="publication_authors">Daniel Hernandez, Antonio Espinosa, David Vazquez, Antonio M. Lopez and  Juan Carlos Moure</span> </div><div class="mfp-hide mfp-with-anim publication-detailHEV2017b publication-detail"> <div class="image_work" id="image_work_HEV2017b"><img class="img-responsive" src="./Pubs/HEV2017b/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">GPU-ACCELERATED REAL-TIME STIXEL COMPUTATION</h3> <span class="publication_authors">Daniel Hernandez, Antonio Espinosa, David Vazquez, Antonio M. Lopez and  Juan Carlos Moure</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">The Stixel World is a medium-level, compact representation of road scenes that abstracts millions of disparity pixels into hundreds or thousands of stixels. The goal of this work is to implement and evaluate a complete multi-stixel estimation pipeline on an embedded, energyefficient, GPU-accelerated device. This work presents a full GPU-accelerated implementation of stixel estimation that produces reliable results at 26 frames per second (real-time) on the Tegra X1 for disparity images of 1024×440 pixels and stixel widths of 5 pixels, and achieves more than 400 frames per second on a high-end Titan X GPU card.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {HEV2017b,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Daniel Hernandez and Antonio Espinosa and David Vazquez and Antonio Lopez and Juan Carlos Moure}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {GPU-accelerated real-time stixel computation},<br> &nbsp;&nbsp; booktitle = {IEEE Winter Conference on Applications of Computer Vision}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2017}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/HEV2016.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2017-01-01"><div class="media"> <a href=".publication-detailGKA2017" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_GKA2017"><img class="img-responsive" src="./Pubs/GKA2017/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2017</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/gka2016.pdf">PIXELVAE: A LATENT VARIABLE MODEL FOR NATURAL IMAGES</a></h3> <h4>5TH INTERNATIONAL CONFERENCE ON LEARNING REPRESENTATIONS (<b>ICLR</b>), 2017</h4> <span class="publication_description">Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and generate samples that preserve global structure but te...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_GKA2017"></span> <span class="publication_authors">Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David Vazquez and  Aaron Courville</span> </div><div class="mfp-hide mfp-with-anim publication-detailGKA2017 publication-detail"> <div class="image_work" id="image_work_GKA2017"><img class="img-responsive" src="./Pubs/GKA2017/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">PIXELVAE: A LATENT VARIABLE MODEL FOR NATURAL IMAGES</h3> <span class="publication_authors">Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David Vazquez and  Aaron Courville</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and generate samples that preserve global structure but tend to suffer from image blurriness. PixelCNNs model sharp contours and details very well, but lack an explicit latent representation and have difficulty modeling large-scale structure in a computationally efficient way. In this paper, we present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. The resulting architecture achieves state-of-the-art log-likelihood on binarized MNIST. We extend PixelVAE to a hierarchy of multiple latent variables at different scales; this hierarchical model achieves competitive likelihood on 64x64 ImageNet and generates high-quality samples on LSUN bedrooms.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {GKA2017,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Ishaan Gulrajani and Kundan Kumar and Faruk Ahmed and Adrien Ali Taiga and Francesco Visin and David Vazquez and Aaron Courville}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {PixelVAE: A Latent Variable Model for Natural Images},<br> &nbsp;&nbsp; booktitle = {5th International Conference on Learning Representations}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2017}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/gka2016.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;BOOK CHAPTERS&quot;]" data-date-publication="2017-01-01"><div class="media"> <a href=".publication-detailLXG2017" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_LXG2017"><img class="img-responsive" src="./Pubs/LXG2017/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2017</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/LXG2016.pdf">FROM VIRTUAL TO REAL WORLD VISUAL PERCEPTION USING DOMAIN ADAPTATION -- THE DPM AS EXAMPLE</a></h3> <h4>BOOK CHAPTER: DOMAIN ADAPTATION IN COMPUTER VISION APPLICATIONS, 2017</h4> <span class="publication_description">Supervised learning tends to produce more accurate classifiers than unsupervised learning in general. This implies that training data is preferred with annotations. When addressing visual perception c...</span> </div><hr style="margin:8px auto"> <span class="label label-danger">Book Chapters</span><span class="label selected" id="selected_LXG2017"></span> <span class="publication_authors">Antonio M. Lopez, Jiaolong Xu, Jose L. Gomez, David Vazquez and  German Ros</span> </div><div class="mfp-hide mfp-with-anim publication-detailLXG2017 publication-detail"> <div class="image_work" id="image_work_LXG2017"><img class="img-responsive" src="./Pubs/LXG2017/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">FROM VIRTUAL TO REAL WORLD VISUAL PERCEPTION USING DOMAIN ADAPTATION -- THE DPM AS EXAMPLE</h3> <span class="publication_authors">Antonio M. Lopez, Jiaolong Xu, Jose L. Gomez, David Vazquez and  German Ros</span> <span class="label label-danger">Book Chapters</span> <span class="label selected">Selected</span> <p class="project_desc">Supervised learning tends to produce more accurate classifiers than unsupervised learning in general. This implies that training data is preferred with annotations. When addressing visual perception challenges, such as localizing certain object classes within an image, the learning of the involved classifiers turns out to be a practical bottleneck. The reason is that, at least, we have to frame object examples with bounding boxes in thousands of images. A priori, the more complex the model is regarding its number of parameters, the more annotated examples are required. This annotation task is performed by human oracles, which ends up in inaccuracies and errors in the annotations (aka ground truth) since the task is inherently very cumbersome and sometimes ambiguous. As an alternative we have pioneered the use of virtual worlds for collecting such annotations automatically and with high precision. However, since the models learned with virtual data must operate in the real world, we still need to perform domain adaptation (DA). In this chapter we revisit the DA of a deformable part-based model (DPM) as an exemplifying case of virtual- to-real-world DA. As a use case, we address the challenge of vehicle detection for driver assistance, using different publicly available virtual-world data. While doing so, we investigate questions such as: how does the domain gap behave due to virtual-vs-real data with respect to dominant object appearance per domain, as well as the role of photo-realism in the virtual world.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INBOOK {LXG2017,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Antonio Lopez and Jiaolong Xu and Jose L. Gomez and David Vazquez and German Ros}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {From Virtual to Real World Visual Perception using Domain Adaptation -- The DPM as Example},<br> &nbsp;&nbsp; booktitle = {Domain Adaptation in Computer Vision Applications}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2017}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/LXG2016.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2017-01-01"><div class="media"> <a href=".publication-detailHEV2017a" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_HEV2017a"><img class="img-responsive" src="./Pubs/HEV2017a/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2017</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/hev2017.pdf">EMBEDDED REAL-TIME STIXEL COMPUTATION</a></h3> <h4>GPU TECHNOLOGY CONFERENCE (<b>GTC</b>), 2017</h4> <span class="publication_description"></span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_HEV2017a"></span> <span class="publication_authors">Daniel Hernandez, Antonio Espinosa, David Vazquez, Antonio M. Lopez and  Juan Carlos Moure</span> </div><div class="mfp-hide mfp-with-anim publication-detailHEV2017a publication-detail"> <div class="image_work" id="image_work_HEV2017a"><img class="img-responsive" src="./Pubs/HEV2017a/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">EMBEDDED REAL-TIME STIXEL COMPUTATION</h3> <span class="publication_authors">Daniel Hernandez, Antonio Espinosa, David Vazquez, Antonio M. Lopez and  Juan Carlos Moure</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc"></p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {HEV2017a,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Daniel Hernandez and Antonio Espinosa and David Vazquez and Antonio Lopez and Juan Carlos Moure}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Embedded Real-time Stixel Computation},<br> &nbsp;&nbsp; booktitle = {GPU Technology Conference}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2017}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/hev2017.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;DEMONSTRATIONS&quot;]" data-date-publication="2017-01-01"><div class="media"> <a href=".publication-detailVBS2017" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_VBS2017"><img class="img-responsive" src="./Pubs/VBS2017/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2017</span></div><div class="media-body"> <h3><a class="ext_link" href="">A BENCHMARK FOR ENDOLUMINAL SCENE SEGMENTATION OF COLONOSCOPY IMAGES</a></h3> <h4>COMPUTER ASSISTED RADIOLOGY AND SURGERY (<b>CARS</b>), 2017</h4> <span class="publication_description">Colorectal cancer (CRC) is the third cause of cancer death worldwide. Currently, the standard approach to reduce CRC-related mortality is to perform regular screening in search for polyps and colonosc...</span> </div><hr style="margin:8px auto"> <span class="label label-warning">Demonstrations</span><span class="label selected" id="selected_VBS2017"></span> <span class="publication_authors">David Vazquez, Jorge Bernal, F. Javier Sanchez, Gloria Fernandez-Esparrach, Antonio M. Lopez, Adriana Romero, Michal Drozdzal and  Aaron Courville</span> </div><div class="mfp-hide mfp-with-anim publication-detailVBS2017 publication-detail"> <div class="image_work" id="image_work_VBS2017"><img class="img-responsive" src="./Pubs/VBS2017/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">A BENCHMARK FOR ENDOLUMINAL SCENE SEGMENTATION OF COLONOSCOPY IMAGES</h3> <span class="publication_authors">David Vazquez, Jorge Bernal, F. Javier Sanchez, Gloria Fernandez-Esparrach, Antonio M. Lopez, Adriana Romero, Michal Drozdzal and  Aaron Courville</span> <span class="label label-warning">Demonstrations</span> <span class="label selected">Selected</span> <p class="project_desc">Colorectal cancer (CRC) is the third cause of cancer death worldwide. Currently, the standard approach to reduce CRC-related mortality is to perform regular screening in search for polyps and colonoscopy is the screening tool of choice. The main limitations of this screening procedure are polyp miss-rate and inability to perform visual assessment of polyp malignancy. These drawbacks can be reduced by designing Decision Support Systems (DSS) aiming to help clinicians in the different stages of the procedure by providing endoluminal scene segmentation. Thus, in this paper, we introduce an extended benchmark of colonoscopy image, with the hope of establishing a new strong benchmark for colonoscopy image analysis research. We provide new baselines on this dataset by training standard fully convolutional networks (FCN) for semantic segmentation and significantly outperforming, without any further post-processing, prior results in endoluminal scene segmentation.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {VBS2017,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {David Vazquez and Jorge Bernal and F. Javier Sanchez and Gloria Fernandez-Esparrach and Antonio Lopez and Adriana Romero and Michal Drozdzal and Aaron Courville}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {A Benchmark for Endoluminal Scene Segmentation of Colonoscopy Images},<br> &nbsp;&nbsp; booktitle = {Computer Assisted Radiology and Surgery}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2017}<br>}</div><a class="ext_link" href=""><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;BOOK CHAPTERS&quot;]" data-date-publication="2017-01-01"><div class="media"> <a href=".publication-detailGVE2017" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_GVE2017"><img class="img-responsive" src="./Pubs/GVE2017/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2017</span></div><div class="media-body"> <h3><a class="ext_link" href="">VISION-BASED ADVANCED DRIVER ASSISTANCE SYSTEMS</a></h3> <h4>BOOK CHAPTER: COMPUTER VISION IN VEHICLE TECHNOLOGY: LAND, SEA, AND AIR, 2017</h4> <span class="publication_description"></span> </div><hr style="margin:8px auto"> <span class="label label-danger">Book Chapters</span><span class="label selected" id="selected_GVE2017"></span> <span class="publication_authors">David Geronimo, David Vazquez and  Arturo de la Escalera</span> </div><div class="mfp-hide mfp-with-anim publication-detailGVE2017 publication-detail"> <div class="image_work" id="image_work_GVE2017"><img class="img-responsive" src="./Pubs/GVE2017/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">VISION-BASED ADVANCED DRIVER ASSISTANCE SYSTEMS</h3> <span class="publication_authors">David Geronimo, David Vazquez and  Arturo de la Escalera</span> <span class="label label-danger">Book Chapters</span> <span class="label selected">Selected</span> <p class="project_desc"></p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INBOOK {GVE2017,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {David Geronimo and David Vazquez and Arturo de la Escalera}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Vision-Based Advanced Driver Assistance Systems},<br> &nbsp;&nbsp; booktitle = {Computer Vision in Vehicle Technology: Land, Sea, and Air}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2017}<br>}</div><a class="ext_link" href=""><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;BOOK CHAPTERS&quot;]" data-date-publication="2017-01-01"><div class="media"> <a href=".publication-detailRSV2017" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_RSV2017"><img class="img-responsive" src="./Pubs/RSV2017/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2017</span></div><div class="media-body"> <h3><a class="ext_link" href="">SEMANTIC SEGMENTATION OF URBAN SCENES VIA DOMAIN ADAPTATION OF SYNTHIA</a></h3> <h4>BOOK CHAPTER: DOMAIN ADAPTATION IN COMPUTER VISION APPLICATIONS, 2017</h4> <span class="publication_description">Vision-based semantic segmentation in urban scenarios is a key functionality for autonomous driving. Recent revolutionary results of deep convolutional neural networks (DCNNs) foreshadow the advent of...</span> </div><hr style="margin:8px auto"> <span class="label label-danger">Book Chapters</span><span class="label selected" id="selected_RSV2017"></span> <span class="publication_authors">German Ros, Laura Sellart, Gabriel Villalonga, Elias Maidanik, Francisco Molero, Marc Garcia, Adriana Cedeño, Francisco Perez, Didier Ramirez, Eduardo Escobar, Jose Luis Gomez, David Vazquez and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailRSV2017 publication-detail"> <div class="image_work" id="image_work_RSV2017"><img class="img-responsive" src="./Pubs/RSV2017/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">SEMANTIC SEGMENTATION OF URBAN SCENES VIA DOMAIN ADAPTATION OF SYNTHIA</h3> <span class="publication_authors">German Ros, Laura Sellart, Gabriel Villalonga, Elias Maidanik, Francisco Molero, Marc Garcia, Adriana Cedeño, Francisco Perez, Didier Ramirez, Eduardo Escobar, Jose Luis Gomez, David Vazquez and  Antonio M. Lopez</span> <span class="label label-danger">Book Chapters</span> <span class="label selected">Selected</span> <p class="project_desc">Vision-based semantic segmentation in urban scenarios is a key functionality for autonomous driving. Recent revolutionary results of deep convolutional neural networks (DCNNs) foreshadow the advent of reliable classifiers to perform such visual tasks. However, DCNNs require learning of many parameters from raw images; thus, having a sufficient amount of diverse images with class annotations is needed. These annotations are obtained via cumbersome, human labour which is particularly challenging for semantic segmentation since pixel-level annotations are required. In this chapter, we propose to use a combination of a virtual world to automatically generate realistic synthetic images with pixel-level annotations, and domain adaptation to transfer the models learnt to correctly operate in real scenarios. We address the question of how useful synthetic data can be for semantic segmentation – in particular, when using a DCNN paradigm. In order to answer this question we have generated a synthetic collection of diverse urban images, named SYNTHIA, with automatically generated class annotations and object identifiers. We use SYNTHIA in combination with publicly available real-world urban images with manually provided annotations. Then, we conduct experiments with DCNNs that show that combining SYNTHIA with simple domain adaptation techniques in the training stage significantly improves performance on semantic segmentation.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INBOOK {RSV2017,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {German Ros and Laura Sellart and Gabriel Villalonga and Elias Maidanik and Francisco Molero and Marc Garcia and Adriana Cedeño and Francisco Perez and Didier Ramirez and Eduardo Escobar and Jose Luis Gomez and David Vazquez and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Semantic Segmentation of Urban Scenes via Domain Adaptation of SYNTHIA},<br> &nbsp;&nbsp; booktitle = {Domain Adaptation in Computer Vision Applications}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2017}<br>}</div><a class="ext_link" href=""><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;JOURNAL PAPERS&quot;]" data-date-publication="2016-01-01"><div class="media"> <a href=".publication-detailXRV2016" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_XRV2016"><img class="img-responsive" src="./Pubs/XRV2016/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2016</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/XRV2016.pdf">HIERARCHICAL ADAPTIVE STRUCTURAL SVM FOR DOMAIN ADAPTATION</a></h3> <h4>INTERNATIONAL JOURNAL OF COMPUTER VISION (<b>IJCV</b>), 119(2), PP. 159-178, 2016</h4> <span class="publication_description">A key topic in classification is the accuracy loss produced when the data distribution in the training (source) domain differs from that in the testing (target) domain. This is being recognized as a v...</span> </div><hr style="margin:8px auto"> <span class="label label-success">Journal papers</span><span class="label selected" id="selected_XRV2016">Selected</span> <span class="publication_authors">Jiaolong Xu, Sebastian Ramos, David Vazquez and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailXRV2016 publication-detail"> <div class="image_work" id="image_work_XRV2016"><img class="img-responsive" src="./Pubs/XRV2016/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">HIERARCHICAL ADAPTIVE STRUCTURAL SVM FOR DOMAIN ADAPTATION</h3> <span class="publication_authors">Jiaolong Xu, Sebastian Ramos, David Vazquez and  Antonio M. Lopez</span> <span class="label label-success">Journal papers</span> <span class="label selected">Selected</span> <p class="project_desc">A key topic in classification is the accuracy loss produced when the data distribution in the training (source) domain differs from that in the testing (target) domain. This is being recognized as a very relevant problem for many
                  computer vision tasks such as image classification, object detection, and object category recognition. In this paper, we present a novel domain adaptation method that leverages multiple target domains (or sub-domains) in a hierarchical adaptation tree. The core idea is to exploit the commonalities and differences of the jointly considered target domains.
                  Given the relevance of structural SVM (SSVM) classifiers, we apply our idea to the adaptive SSVM (A-SSVM), which only requires the target domain samples together with the existing source-domain classifier for performing the desired adaptation. Altogether, we term our proposal as hierarchical A-SSVM (HA-SSVM).
                  As proof of concept we use HA-SSVM for pedestrian detection, object category recognition and face recognition. In the former we apply HA-SSVM to the deformable partbased model (DPM) while in the rest HA-SSVM is applied to multi-category classifiers. We will show how HA-SSVM is effective in increasing the detection/recognition accuracy with respect to adaptation strategies that ignore the structure of the target data. Since, the sub-domains of the target data are not always known a priori, we shown how HA-SSVM can incorporate sub-domain discovery for object category recognition.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@ARTICLE {XRV2016,<br> &nbsp;&nbsp; author &nbsp;= {Jiaolong Xu and Sebastian Ramos and David Vazquez and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Hierarchical Adaptive Structural SVM for Domain Adaptation},<br> &nbsp;&nbsp; journal = {International Journal of Computer Vision}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp; = {2016}, <br> &nbsp;&nbsp; volume = {119}, <br> &nbsp;&nbsp; issue &nbsp;&nbsp;&nbsp; = {2}, <br> &nbsp;&nbsp; pages &nbsp;&nbsp; = {159-178}  <br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/XRV2016.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2016-01-01"><div class="media"> <a href=".publication-detailXVM2016" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_XVM2016"><img class="img-responsive" src="./Pubs/XVM2016/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2016</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/XVM2016.pdf">HIERARCHICAL ONLINE DOMAIN ADAPTATION OF DEFORMABLE PART-BASED MODELS</a></h3> <h4>IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (<b>ICRA</b>), 2016</h4> <span class="publication_description">We propose an online domain adaptation method for the deformable part-based model (DPM). The online domain adaptation is based on a two-level hierarchical adaptation tree, which consists of instance d...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_XVM2016"></span> <span class="publication_authors">Jiaolong Xu, David Vazquez, Krystian Mikolajczyk and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailXVM2016 publication-detail"> <div class="image_work" id="image_work_XVM2016"><img class="img-responsive" src="./Pubs/XVM2016/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">HIERARCHICAL ONLINE DOMAIN ADAPTATION OF DEFORMABLE PART-BASED MODELS</h3> <span class="publication_authors">Jiaolong Xu, David Vazquez, Krystian Mikolajczyk and  Antonio M. Lopez</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">We propose an online domain adaptation method for the deformable part-based model (DPM). The online domain adaptation is based on a two-level hierarchical adaptation tree, which consists of instance detectors in the leaf nodes and a category detector at the root node. Moreover, combined with a multiple object tracking procedure (MOT), our proposal neither requires target-domain annotated data nor revisiting the source-domain data for performing the source-to-target domain adaptation of the DPM. From a practical point of view this means that, given a source-domain DPM and new video for training on a new domain without object annotations, our procedure outputs a new DPM adapted to the domain represented by the video. As proof-of-concept we apply our proposal to the challenging task of pedestrian detection. In this case, each instance detector is an exemplar classifier trained online with only one pedestrian per frame. The pedestrian instances are collected by MOT and the hierarchical model is constructed dynamically according to the pedestrian trajectories. Our experimental results show that the adapted detector achieves the accuracy of recent supervised domain adaptation methods (i.e., requiring manually annotated targetdomain data), and improves the source detector more than 10 percentage points.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {XVM2016,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Jiaolong Xu and David Vazquez and Krystian Mikolajczyk and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Hierarchical online domain adaptation of deformable part-based models},<br> &nbsp;&nbsp; booktitle = {IEEE International Conference on Robotics and Automation}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2016}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/XVM2016.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2016-01-01"><div class="media"> <a href=".publication-detailCSM2016" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_CSM2016"><img class="img-responsive" src="./Pubs/CSM2016/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2016</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/CSM2016.pdf">GPU-BASED PEDESTRIAN DETECTION FOR AUTONOMOUS DRIVING</a></h3> <h4>GPU TECHNOLOGY CONFERENCE (<b>GTC</b>), 2016</h4> <span class="publication_description">Pedestrian detection for autonomous driving is one of the hardest tasks within computer vision, and involves huge computational costs. Obtaining acceptable real-time performance, measured in frames pe...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_CSM2016"></span> <span class="publication_authors">Victor Campmany, Sergio Silva, Juan Carlos Moure, Toni Espinosa, David Vazquez and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailCSM2016 publication-detail"> <div class="image_work" id="image_work_CSM2016"><img class="img-responsive" src="./Pubs/CSM2016/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">GPU-BASED PEDESTRIAN DETECTION FOR AUTONOMOUS DRIVING</h3> <span class="publication_authors">Victor Campmany, Sergio Silva, Juan Carlos Moure, Toni Espinosa, David Vazquez and  Antonio M. Lopez</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">Pedestrian detection for autonomous driving is one of the hardest tasks within computer vision, and involves huge computational costs. Obtaining acceptable real-time performance, measured in frames per second (fps), for the most advanced algorithms is nowadays a hard challenge. Taking the work in [1] as our baseline, we propose a CUDA implementation of a pedestrian detection system that includes LBP and HOG as feature descriptors and SVM and Random forest as classifiers. We introduce significant algorithmic adjustments and optimizations to adapt the problem to the NVIDIA GPU architecture. The aim is to deploy a real-time system providing reliable results.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {CSM2016,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Victor Campmany and Sergio Silva and Juan Carlos Moure and Toni Espinosa and David Vazquez and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {GPU-based pedestrian detection for autonomous driving},<br> &nbsp;&nbsp; booktitle = {GPU Technology Conference}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2016}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/CSM2016.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2016-01-01"><div class="media"> <a href=".publication-detailHME2016" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_HME2016"><img class="img-responsive" src="./Pubs/HME2016/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2016</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/HME2016.pdf">REAL-TIME 3D RECONSTRUCTION FOR AUTONOMOUS DRIVING VIA SEMI-GLOBAL MATCHING</a></h3> <h4>GPU TECHNOLOGY CONFERENCE (<b>GTC</b>), 2016</h4> <span class="publication_description">Robust and dense computation of depth information from stereo-camera systems is a computationally demanding requirement for real-time autonomous driving. Semi-Global Matching (SGM) [1] approximates he...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_HME2016"></span> <span class="publication_authors">Daniel Hernandez, Juan Carlos Moure, Toni Espinosa, Alejandro Chacon, David Vazquez and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailHME2016 publication-detail"> <div class="image_work" id="image_work_HME2016"><img class="img-responsive" src="./Pubs/HME2016/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">REAL-TIME 3D RECONSTRUCTION FOR AUTONOMOUS DRIVING VIA SEMI-GLOBAL MATCHING</h3> <span class="publication_authors">Daniel Hernandez, Juan Carlos Moure, Toni Espinosa, Alejandro Chacon, David Vazquez and  Antonio M. Lopez</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">Robust and dense computation of depth information from stereo-camera systems is a computationally demanding requirement for real-time autonomous driving. Semi-Global Matching (SGM) [1] approximates heavy-computation global algorithms results but with lower computational complexity, therefore it is a good candidate for a real-time implementation. SGM minimizes energy along several 1D paths across the image. The aim of this work is to provide a real-time system producing reliable results on energy-efficient hardware. Our design runs on a NVIDIA Titan X GPU at 104.62 FPS and on a NVIDIA Drive PX at 6.7 FPS, promising for real-time platforms</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {HME2016,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Daniel Hernandez and Juan Carlos Moure and Toni Espinosa and Alejandro Chacon and David Vazquez and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Real-time 3D Reconstruction for Autonomous Driving via Semi-Global Matching},<br> &nbsp;&nbsp; booktitle = {GPU Technology Conference}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2016}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/HME2016.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2016-01-01"><div class="media"> <a href=".publication-detailRSM2016" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_RSM2016"><img class="img-responsive" src="./Pubs/RSM2016/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2016</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/RSM2016.pdf">THE SYNTHIA DATASET: A LARGE COLLECTION OF SYNTHETIC IMAGES FOR SEMANTIC SEGMENTATION OF URBAN SCENES</a></h3> <h4>29TH IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (<b>CVPR</b>), 2016</h4> <span class="publication_description">Vision-based semantic segmentation in urban scenarios is a key functionality for autonomous driving. The irruption of deep convolutional neural networks (DCNNs) allows to foresee obtaining reliable cl...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_RSM2016"></span> <span class="publication_authors">German Ros, Laura Sellart, Joanna Materzynska, David Vazquez and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailRSM2016 publication-detail"> <div class="image_work" id="image_work_RSM2016"><img class="img-responsive" src="./Pubs/RSM2016/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">THE SYNTHIA DATASET: A LARGE COLLECTION OF SYNTHETIC IMAGES FOR SEMANTIC SEGMENTATION OF URBAN SCENES</h3> <span class="publication_authors">German Ros, Laura Sellart, Joanna Materzynska, David Vazquez and  Antonio M. Lopez</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">Vision-based semantic segmentation in urban scenarios is a key functionality for autonomous driving. The irruption of deep convolutional neural networks (DCNNs) allows to foresee obtaining reliable classifiers to perform such a visual task. However, DCNNs require to learn many parameters from raw images; thus, having a sufficient amount of diversified images with this class annotations is needed. These annotations are obtained by a human cumbersome labour specially challenging for semantic segmentation, since pixel-level annotations are required. In this paper, we propose to use a virtual world for automatically generating realistic synthetic images with pixel-level annotations. Then, we address the question of how useful can be such data for the task of semantic segmentation; in particular, when using a DCNN paradigm. In order to answer this question we have generated a synthetic diversified collection of urban images, named SynthCity, with automatically generated class annotations. We use SynthCity in combination with publicly available real-world urban images with manually provided annotations. Then, we conduct experiments on a DCNN setting that show how the inclusion of SynthCity in the training stage significantly improves the performance of the semantic segmentation task</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {RSM2016,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {German Ros and Laura Sellart and Joanna Materzynska and David Vazquez and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes},<br> &nbsp;&nbsp; booktitle = {29th IEEE Conference on Computer Vision and Pattern Recognition}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2016}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/RSM2016.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2016-01-01"><div class="media"> <a href=".publication-detailHCE2016a" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_HCE2016a"><img class="img-responsive" src="./Pubs/HCE2016a/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2016</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/HCE2016.pdf">EMBEDDED REAL-TIME STEREO ESTIMATION VIA SEMI-GLOBAL MATCHING ON THE GPU</a></h3> <h4>16TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL SCIENCE (<b>ICCS</b>), 2016</h4> <span class="publication_description">Dense, robust and real-time computation of depth information from stereo-camera systems is a computationally demanding requirement for robotics, advanced driver assistance systems (ADAS) and autonomou...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_HCE2016a"></span> <span class="publication_authors">Daniel Hernandez, Alejandro Chacon, Antonio Espinosa, David Vazquez, Juan Carlos Moure and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailHCE2016a publication-detail"> <div class="image_work" id="image_work_HCE2016a"><img class="img-responsive" src="./Pubs/HCE2016a/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">EMBEDDED REAL-TIME STEREO ESTIMATION VIA SEMI-GLOBAL MATCHING ON THE GPU</h3> <span class="publication_authors">Daniel Hernandez, Alejandro Chacon, Antonio Espinosa, David Vazquez, Juan Carlos Moure and  Antonio M. Lopez</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">Dense, robust and real-time computation of depth information from stereo-camera systems is a computationally demanding requirement for robotics, advanced driver assistance systems (ADAS) and autonomous vehicles. Semi-Global Matching (SGM) is a widely used algorithm that propagates consistency constraints along several paths across the image. This work presents a real-time system producing reliable disparity estimation results on the new embedded energy-efficient GPU devices. Our design runs on a Tegra X1 at 41 frames per second for an image size of 640x480, 128 disparity levels, and using 4 path directions for the SGM method.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {HCE2016a,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Daniel Hernandez and Alejandro Chacon and Antonio Espinosa and David Vazquez and Juan Carlos Moure and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Embedded real-time stereo estimation via Semi-Global Matching on the GPU},<br> &nbsp;&nbsp; booktitle = {16th International Conference on Computational Science}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2016}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/HCE2016.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2016-01-01"><div class="media"> <a href=".publication-detailCSE2016" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_CSE2016"><img class="img-responsive" src="./Pubs/CSE2016/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2016</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/CSE2016.pdf">GPU-BASED PEDESTRIAN DETECTION FOR AUTONOMOUS DRIVING</a></h3> <h4>16TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL SCIENCE (<b>ICCS</b>), 2016</h4> <span class="publication_description">We propose a real-time pedestrian detection system for the embedded Nvidia Tegra X1 GPU-CPU hybrid platform. The pipeline is composed by the following state-of-the-art algorithms: Histogram of Local B...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_CSE2016"></span> <span class="publication_authors">Victor Campmany, Sergio Silva, Antonio Espinosa, Juan Carlos Moure, David Vazquez and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailCSE2016 publication-detail"> <div class="image_work" id="image_work_CSE2016"><img class="img-responsive" src="./Pubs/CSE2016/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">GPU-BASED PEDESTRIAN DETECTION FOR AUTONOMOUS DRIVING</h3> <span class="publication_authors">Victor Campmany, Sergio Silva, Antonio Espinosa, Juan Carlos Moure, David Vazquez and  Antonio M. Lopez</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">We propose a real-time pedestrian detection system for the embedded Nvidia Tegra X1 GPU-CPU hybrid platform. The pipeline is composed by the following state-of-the-art algorithms: Histogram of Local Binary Patterns (LBP) and Histograms of Oriented Gradients (HOG) features extracted from the input image; Pyramidal Sliding Window technique for foreground segmentation; and Support Vector Machine (SVM) for classification. Results show a 8x speedup in the target Tegra X1 platform and a better performance/watt ratio than desktop CUDA platforms in study.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {CSE2016,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Victor Campmany and Sergio Silva and Antonio Espinosa and Juan Carlos Moure and David Vazquez and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {GPU-based pedestrian detection for autonomous driving},<br> &nbsp;&nbsp; booktitle = {16th International Conference on Computational Science}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2016}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/CSE2016.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2016-01-01"><div class="media"> <a href=".publication-detailASP2016" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_ASP2016"><img class="img-responsive" src="./Pubs/ASP2016/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2016</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/ASP2016.pdf">COMPARISON OF TWO NON-LINEAR MODEL-BASED CONTROL STRATEGIES FOR AUTONOMOUS VEHICLES</a></h3> <h4>24TH MEDITERRANEAN CONFERENCE ON CONTROL AND AUTOMATION (<b>MED</b>), 2016</h4> <span class="publication_description">This paper presents the comparison of two nonlinear model-based control strategies for autonomous cars. A control oriented model of vehicle based on a bicycle model is used. The two control strategies...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_ASP2016"></span> <span class="publication_authors">Eugenio Alcala, Laura Sellart, Vicenc Puig, Joseba Quevedo, Jordi Saludes, David Vazquez and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailASP2016 publication-detail"> <div class="image_work" id="image_work_ASP2016"><img class="img-responsive" src="./Pubs/ASP2016/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">COMPARISON OF TWO NON-LINEAR MODEL-BASED CONTROL STRATEGIES FOR AUTONOMOUS VEHICLES</h3> <span class="publication_authors">Eugenio Alcala, Laura Sellart, Vicenc Puig, Joseba Quevedo, Jordi Saludes, David Vazquez and  Antonio M. Lopez</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">This paper presents the comparison of two nonlinear model-based control strategies for autonomous cars. A control oriented model of vehicle based on a bicycle model is used. The two control strategies use a model reference approach. Using this approach, the error dynamics model is developed. Both controllers receive as input the longitudinal, lateral and orientation errors generating as control outputs the steering angle and the velocity of the vehicle. The first control approach is based on a non-linear control law that is designed by means of the Lyapunov direct approach. The second approach is based on a sliding mode-control that defines a set of sliding surfaces over which the error trajectories will converge. The main advantage of the sliding-control technique is the robustness against non-linearities and parametric uncertainties in the model. However, the main drawback of first order sliding mode is the chattering, so it has been implemented a high order sliding mode control. To test and compare the proposed control strategies, different path following scenarios are used in simulation.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {ASP2016,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Eugenio Alcala and Laura Sellart and Vicenc Puig and Joseba Quevedo and Jordi Saludes and David Vazquez and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Comparison of two non-linear model-based control strategies for autonomous vehicles},<br> &nbsp;&nbsp; booktitle = {24th Mediterranean Conference on Control and Automation}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2016}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/ASP2016.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;JOURNAL PAPERS&quot;]" data-date-publication="2016-01-01"><div class="media"> <a href=".publication-detailGFS2016" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_GFS2016"><img class="img-responsive" src="./Pubs/GFS2016/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2016</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/GFS2016.pdf">PEDESTRIAN DETECTION AT DAY/NIGHT TIME WITH VISIBLE AND FIR CAMERAS: A COMPARISON</a></h3> <h4>SENSORS (<b>SENS</b>), 16(6), PP. 820, 2016</h4> <span class="publication_description">Despite all the significant advances in pedestrian detection brought by computer vision for driving assistance, it is still a challenging problem. One reason is the extremely varying lighting conditio...</span> </div><hr style="margin:8px auto"> <span class="label label-success">Journal papers</span><span class="label selected" id="selected_GFS2016">Selected</span> <span class="publication_authors">Alejandro Gonzalez Alzate, Zhijie Fang, Yainuvis Socarras, Joan Serrat, David Vazquez, Jiaolong Xu and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailGFS2016 publication-detail"> <div class="image_work" id="image_work_GFS2016"><img class="img-responsive" src="./Pubs/GFS2016/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">PEDESTRIAN DETECTION AT DAY/NIGHT TIME WITH VISIBLE AND FIR CAMERAS: A COMPARISON</h3> <span class="publication_authors">Alejandro Gonzalez Alzate, Zhijie Fang, Yainuvis Socarras, Joan Serrat, David Vazquez, Jiaolong Xu and  Antonio M. Lopez</span> <span class="label label-success">Journal papers</span> <span class="label selected">Selected</span> <p class="project_desc">Despite all the significant advances in pedestrian detection brought by computer vision for driving assistance, it is still a challenging problem. One reason is the extremely varying lighting conditions under which such a detector should operate, namely day and night time. Recent research has shown that the combination of visible and non-visible imaging modalities may increase detection accuracy, where the infrared spectrum plays a critical role. The goal of this paper is to assess the accuracy gain of different pedestrian models (holistic, part-based, patch-based) when training with images in the far infrared spectrum. Specifically, we want to compare detection accuracy on test images recorded at day and nighttime if trained (and tested) using (a) plain color images, (b) just infrared images and (c) both of them. In order to obtain results for the last item we propose an early fusion approach to combine features from both modalities. We base the evaluation on a new dataset we have built for this purpose as well as on the publicly available KAIST multispectral dataset.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@ARTICLE {GFS2016,<br> &nbsp;&nbsp; author &nbsp;= {Alejandro Gonzalez Alzate and Zhijie Fang and Yainuvis Socarras and Joan Serrat and David Vazquez and Jiaolong Xu and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Pedestrian Detection at Day/Night Time with Visible and FIR Cameras: A Comparison},<br> &nbsp;&nbsp; journal = {Sensors}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp; = {2016}, <br> &nbsp;&nbsp; volume = {16}, <br> &nbsp;&nbsp; issue &nbsp;&nbsp;&nbsp; = {6}, <br> &nbsp;&nbsp; pages &nbsp;&nbsp; = {820}  <br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/GFS2016.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;THESES&quot;]" data-date-publication="2016-01-01"><div class="media"> <a href=".publication-detailHCE2016b" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_HCE2016b"><img class="img-responsive" src="./Pubs/HCE2016b/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2016</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/HCE2016b.pdf">STEREO MATCHING USING SGM ON THE GPU</a></h3> <h4>PROGRAMMING AND TUNING MASSIVELY PARALLEL SYSTEMS (<b>PUMPS</b>), 2016</h4> <span class="publication_description">Dense, robust and real-time computation of depth information from stereo-camera systems is a computationally demanding requirement for robotics, advanced driver assistance systems (ADAS) and autonomou...</span> </div><hr style="margin:8px auto"> <span class="label label-default">Theses</span><span class="label selected" id="selected_HCE2016b"></span> <span class="publication_authors">Daniel Hernandez, Alejandro Chacon, Antonio Espinosa, David Vazquez, Juan Carlos Moure and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailHCE2016b publication-detail"> <div class="image_work" id="image_work_HCE2016b"><img class="img-responsive" src="./Pubs/HCE2016b/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">STEREO MATCHING USING SGM ON THE GPU</h3> <span class="publication_authors">Daniel Hernandez, Alejandro Chacon, Antonio Espinosa, David Vazquez, Juan Carlos Moure and  Antonio M. Lopez</span> <span class="label label-default">Theses</span> <span class="label selected">Selected</span> <p class="project_desc">Dense, robust and real-time computation of depth information from stereo-camera systems is a computationally demanding requirement for robotics, advanced driver assistance systems (ADAS) and autonomous vehicles. Semi-Global Matching (SGM) is a widely used algorithm that propagates consistency constraints along several paths across the image. This work presents a real-time system producing reliable disparity estimation results on the new embedded energy efficient GPU devices. Our design runs on a Tegra X1 at 42 frames per second (fps) for an image size of 640x480, 128 disparity levels, and using 4 path directions for the SGM method.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {HCE2016b,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Daniel Hernandez and Alejandro Chacon and Antonio Espinosa and David Vazquez and Juan Carlos Moure and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Stereo Matching using SGM on the GPU},<br> &nbsp;&nbsp; booktitle = {Programming and Tuning Massively Parallel Systems}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2016}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/HCE2016b.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;JOURNAL PAPERS&quot;]" data-date-publication="2016-01-01"><div class="media"> <a href=".publication-detailGVL2016" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_GVL2016"><img class="img-responsive" src="./Pubs/GVL2016/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2016</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/GVL2016.pdf">ON-BOARD OBJECT DETECTION: MULTICUE, MULTIMODAL, AND MULTIVIEW RANDOM FOREST OF LOCAL EXPERTS</a></h3> <h4>IEEE TRANSACTIONS ON CYBERNETICS (<b>CYBER</b>), (99), PP. 1-11, 2016</h4> <span class="publication_description">Despite recent significant advances, object detection continues to be an extremely challenging problem in real scenarios. In order to develop a detector that successfully operates under these conditio...</span> </div><hr style="margin:8px auto"> <span class="label label-success">Journal papers</span><span class="label selected" id="selected_GVL2016">Selected</span> <span class="publication_authors">Alejandro Gonzalez Alzate, David Vazquez, Antonio M. Lopez and  Jaume Amores</span> </div><div class="mfp-hide mfp-with-anim publication-detailGVL2016 publication-detail"> <div class="image_work" id="image_work_GVL2016"><img class="img-responsive" src="./Pubs/GVL2016/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">ON-BOARD OBJECT DETECTION: MULTICUE, MULTIMODAL, AND MULTIVIEW RANDOM FOREST OF LOCAL EXPERTS</h3> <span class="publication_authors">Alejandro Gonzalez Alzate, David Vazquez, Antonio M. Lopez and  Jaume Amores</span> <span class="label label-success">Journal papers</span> <span class="label selected">Selected</span> <p class="project_desc">Despite recent significant advances, object detection continues to be an extremely challenging problem in real scenarios. In order to develop a detector that successfully operates under these conditions, it becomes critical to leverage upon multiple cues, multiple imaging modalities, and a strong multiview (MV) classifier that accounts for different object views and poses. In this paper, we provide an extensive evaluation that gives insight into how each of these aspects (multicue, multimodality, and strong MV classifier) affect accuracy both individually and when integrated together. In the multimodality component, we explore the fusion of RGB and depth maps obtained by high-definition light detection and ranging, a type of modality that is starting to receive increasing attention. As our analysis reveals, although all the aforementioned aspects significantly help in improving the accuracy, the fusion of visible spectrum and depth information allows to boost the accuracy by a much larger margin. The resulting detector not only ranks among the top best performers in the challenging KITTI benchmark, but it is built upon very simple blocks that are easy to implement and computationally efficient.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@ARTICLE {GVL2016,<br> &nbsp;&nbsp; author &nbsp;= {Alejandro Gonzalez Alzate and David Vazquez and Antonio Lopez and Jaume Amores}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {On-Board Object Detection: Multicue, Multimodal, and Multiview Random Forest of Local Experts},<br> &nbsp;&nbsp; journal = {IEEE Transactions on cybernetics}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp; = {2016}, <br> &nbsp;&nbsp; volume = {}, <br> &nbsp;&nbsp; issue &nbsp;&nbsp;&nbsp; = {99}, <br> &nbsp;&nbsp; pages &nbsp;&nbsp; = {1-11}  <br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/GVL2016.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;DEMONSTRATIONS&quot;]" data-date-publication="2016-01-01"><div class="media"> <a href=".publication-detailJDV2016" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_JDV2016"><img class="img-responsive" src="./Pubs/JDV2016/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2016</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/jdv2016.pdf">THE ONE HUNDRED LAYERS TIRAMISU: FULLY CONVOLUTIONAL DENSENETS FOR SEMANTIC SEGMENTATION</a></h3> <h4>ARXIV (<b></b>), 2016</h4> <span class="publication_description">State-of-the-art approaches for semantic image segmentation are built on Convolutional Neural Networks (CNNs). The typical segmentation architecture is composed of (a) a downsampling path responsible ...</span> </div><hr style="margin:8px auto"> <span class="label label-warning">Demonstrations</span><span class="label selected" id="selected_JDV2016"></span> <span class="publication_authors">Simon Jégou, Michal Drozdzal, David Vazquez, Adriana Romero and  Yoshua Bengio</span> </div><div class="mfp-hide mfp-with-anim publication-detailJDV2016 publication-detail"> <div class="image_work" id="image_work_JDV2016"><img class="img-responsive" src="./Pubs/JDV2016/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">THE ONE HUNDRED LAYERS TIRAMISU: FULLY CONVOLUTIONAL DENSENETS FOR SEMANTIC SEGMENTATION</h3> <span class="publication_authors">Simon Jégou, Michal Drozdzal, David Vazquez, Adriana Romero and  Yoshua Bengio</span> <span class="label label-warning">Demonstrations</span> <span class="label selected">Selected</span> <p class="project_desc">State-of-the-art approaches for semantic image segmentation are built on Convolutional Neural Networks (CNNs). The typical segmentation architecture is composed of (a) a downsampling path responsible for extracting coarse semantic features, followed by (b) an upsampling path trained to recover the input image resolution at the output of the model and, optionally, (c) a post-processing module (e.g. Conditional Random Fields) to refine the model predictions.

                  Recently, a new CNN architecture, Densely Connected Convolutional Networks (DenseNets), has shown excellent results on image classification tasks. The idea of DenseNets is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train.

                  In this paper, we extend DenseNets to deal with the problem of semantic segmentation. We achieve state-of-the-art results on urban scene benchmark datasets such as CamVid and Gatech, without any further post-processing module nor pretraining. Moreover, due to smart construction of the model, our approach has much less parameters than currently published best entries for these datasets.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {JDV2016,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Simon Jégou and Michal Drozdzal and David Vazquez and Adriana Romero and Yoshua Bengio}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation},<br> &nbsp;&nbsp; booktitle = {arXiv}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2016}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/jdv2016.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;DEMONSTRATIONS&quot;]" data-date-publication="2016-01-01"><div class="media"> <a href=".publication-detailMVJ2016" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_MVJ2016"><img class="img-responsive" src="./Pubs/MVJ2016/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2016</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/mvj2016.pdf">NODE-ADAPT, PATH-ADAPT AND TREE-ADAPT:MODEL-TRANSFER DOMAIN ADAPTATION FOR RANDOM FOREST</a></h3> <h4>ARXIV (<b></b>), 2016</h4> <span class="publication_description">Random Forest (RF) is a successful paradigm for learning classifiers due to its ability to learn from large feature spaces and seamlessly integrate multi-class classification, as well as the achieved ...</span> </div><hr style="margin:8px auto"> <span class="label label-warning">Demonstrations</span><span class="label selected" id="selected_MVJ2016"></span> <span class="publication_authors">Azadeh S. Mozafari, David Vazquez, Mansour Jamzad and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailMVJ2016 publication-detail"> <div class="image_work" id="image_work_MVJ2016"><img class="img-responsive" src="./Pubs/MVJ2016/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">NODE-ADAPT, PATH-ADAPT AND TREE-ADAPT:MODEL-TRANSFER DOMAIN ADAPTATION FOR RANDOM FOREST</h3> <span class="publication_authors">Azadeh S. Mozafari, David Vazquez, Mansour Jamzad and  Antonio M. Lopez</span> <span class="label label-warning">Demonstrations</span> <span class="label selected">Selected</span> <p class="project_desc">Random Forest (RF) is a successful paradigm for learning classifiers due to its ability to learn from large feature spaces and seamlessly integrate multi-class classification, as well as the achieved accuracy and processing efficiency. However, as many other classifiers, RF requires domain adaptation (DA) provided that there is a mismatch between the training (source) and testing (target) domains which provokes classification degradation. Consequently, different RF-DA methods have been proposed, which not only require target-domain samples but revisiting the source-domain ones, too. As novelty, we propose three inherently different methods (Node-Adapt, Path-Adapt and Tree-Adapt) that only require the learned source-domain RF and a relatively few target-domain samples for DA, i.e. source-domain samples do not need to be available. To assess the performance of our proposals we focus on image-based object detection, using the pedestrian detection problem as challenging proof-of-concept. Moreover, we use the RF with expert nodes because it is a competitive patch-based pedestrian model. We test our Node-, Path- and Tree-Adapt methods in standard benchmarks, showing that DA is largely achieved.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {MVJ2016,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Azadeh S. Mozafari and David Vazquez and Mansour Jamzad and Antonio M. Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Node-Adapt, Path-Adapt and Tree-Adapt:Model-Transfer Domain Adaptation for Random Forest},<br> &nbsp;&nbsp; booktitle = {arXiv}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2016}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/mvj2016.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;DEMONSTRATIONS&quot;]" data-date-publication="2016-01-01"><div class="media"> <a href=".publication-detailVBS2016" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_VBS2016"><img class="img-responsive" src="./Pubs/VBS2016/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2016</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/VBS2016.pdf">A BENCHMARK FOR ENDOLUMINAL SCENE SEGMENTATION OF COLONOSCOPY IMAGES</a></h3> <h4>ARXIV (<b></b>), 2016</h4> <span class="publication_description">Colorectal cancer (CRC) is the third cause of cancer death worldwide. Currently, the standard approach to reduce CRC-related mortality is to perform regular screening in search for polyps and colonosc...</span> </div><hr style="margin:8px auto"> <span class="label label-warning">Demonstrations</span><span class="label selected" id="selected_VBS2016"></span> <span class="publication_authors">David Vazquez, Jorge Bernal, F. Javier Sanchez, Gloria Fernandez-Esparrach, Antonio M. Lopez, Adriana Romero, Michal Drozdzal and  Aaron Courville</span> </div><div class="mfp-hide mfp-with-anim publication-detailVBS2016 publication-detail"> <div class="image_work" id="image_work_VBS2016"><img class="img-responsive" src="./Pubs/VBS2016/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">A BENCHMARK FOR ENDOLUMINAL SCENE SEGMENTATION OF COLONOSCOPY IMAGES</h3> <span class="publication_authors">David Vazquez, Jorge Bernal, F. Javier Sanchez, Gloria Fernandez-Esparrach, Antonio M. Lopez, Adriana Romero, Michal Drozdzal and  Aaron Courville</span> <span class="label label-warning">Demonstrations</span> <span class="label selected">Selected</span> <p class="project_desc">Colorectal cancer (CRC) is the third cause of cancer death worldwide. Currently, the standard approach to reduce CRC-related mortality is to perform regular screening in search for polyps and colonoscopy is the screening tool of choice. The main limitations of this screening procedure are polyp miss-rate and inability to perform visual assessment of polyp malignancy. These drawbacks can be reduced by designing Decision Support Systems (DSS) aiming to help clinicians in the different stages of the procedure by providing endoluminal scene segmentation. Thus, in this paper, we introduce an extended benchmark of colonoscopy image, with the hope of establishing a new strong benchmark for colonoscopy image analysis research. We provide new baselines on this dataset by training standard fully convolutional networks (FCN) for semantic segmentation and significantly outperforming, without any further post-processing, prior results in endoluminal scene segmentation.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {VBS2016,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {David Vazquez and Jorge Bernal and F. Javier Sanchez and Gloria Fernandez-Esparrach and Antonio Lopez and Adriana Romero and Michal Drozdzal and Aaron Courville}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {A Benchmark for Endoluminal Scene Segmentation of Colonoscopy Images},<br> &nbsp;&nbsp; booktitle = {arXiv}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2016}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/VBS2016.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2015-01-01"><div class="media"> <a href=".publication-detailGRV2015" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_GRV2015"><img class="img-responsive" src="./Pubs/GRV2015/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2015</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/GRV2015.pdf">SPATIOTEMPORAL STACKED SEQUENTIAL LEARNING FOR PEDESTRIAN DETECTION</a></h3> <h4>PATTERN RECOGNITION AND IMAGE ANALYSIS, PROCEEDINGS OF 7TH IBERIAN CONFERENCE , IBPRIA 2015 (<b>IBPRIA</b>), 2015</h4> <span class="publication_description">Pedestrian classifiers decide which image windows contain a pedestrian. In practice, such classifiers provide a relatively high response at neighbor windows overlapping a pedestrian, while the respons...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_GRV2015"></span> <span class="publication_authors">Alejandro Gonzalez Alzate, Sebastian Ramos, David Vazquez, Antonio M. Lopez and  Jaume Amores</span> </div><div class="mfp-hide mfp-with-anim publication-detailGRV2015 publication-detail"> <div class="image_work" id="image_work_GRV2015"><img class="img-responsive" src="./Pubs/GRV2015/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">SPATIOTEMPORAL STACKED SEQUENTIAL LEARNING FOR PEDESTRIAN DETECTION</h3> <span class="publication_authors">Alejandro Gonzalez Alzate, Sebastian Ramos, David Vazquez, Antonio M. Lopez and  Jaume Amores</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">Pedestrian classifiers decide which image windows contain a pedestrian. In practice, such classifiers provide a relatively high response at neighbor windows overlapping a pedestrian, while the responses around potential false positives are expected to be lower. An analogous reasoning applies for image sequences. If there is a pedestrian located within a frame, the same pedestrian is expected to appear close to the same location in neighbor frames. Therefore, such a location has chances of receiving high classification scores during several frames, while false positives are expected to be more spurious. In this paper we propose to exploit such correlations for improving the accuracy of base pedestrian classifiers. In particular, we propose to use two-stage classifiers which not only rely on the image descriptors required by the base classifiers but also on the response of such base classifiers in a given spatiotemporal neighborhood. More specifically, we train pedestrian classifiers using a stacked sequential learning (SSL) paradigm. We use a new pedestrian dataset we have acquired from a car to evaluate our proposal at different frame rates. We also test on a well known dataset: Caltech. The obtained results show that our SSL proposal boosts detection accuracy significantly with a minimal impact on the computational cost. Interestingly, SSL improves more the accuracy at the most dangerous situations, i.e. when a pedestrian is close to the camera.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {GRV2015,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Alejandro Gonzalez Alzate and Sebastian Ramos and David Vazquez and Antonio Lopez and Jaume Amores}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Spatiotemporal Stacked Sequential Learning for Pedestrian Detection},<br> &nbsp;&nbsp; booktitle = {Pattern Recognition and Image Analysis, Proceedings of 7th Iberian Conference , ibPRIA 2015}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2015}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/GRV2015.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2015-01-01"><div class="media"> <a href=".publication-detailRRG2015" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_RRG2015"><img class="img-responsive" src="./Pubs/RRG2015/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2015</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/rrg2015.pdf">VISION-BASED OFFLINE-ONLINE PERCEPTION PARADIGM FOR AUTONOMOUS DRIVING</a></h3> <h4>IEEE WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION WACV2015 (<b>WACV</b>), 2015</h4> <span class="publication_description">Autonomous driving is a key factor for future mobility. Properly perceiving the environment of the vehicles is essential for a safe driving, which requires computing accurate geometric and semantic in...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_RRG2015"></span> <span class="publication_authors">German Ros, Sebastian Ramos, Manuel Granados, Amir Bakhtiary, David Vazquez and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailRRG2015 publication-detail"> <div class="image_work" id="image_work_RRG2015"><img class="img-responsive" src="./Pubs/RRG2015/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">VISION-BASED OFFLINE-ONLINE PERCEPTION PARADIGM FOR AUTONOMOUS DRIVING</h3> <span class="publication_authors">German Ros, Sebastian Ramos, Manuel Granados, Amir Bakhtiary, David Vazquez and  Antonio M. Lopez</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">Autonomous driving is a key factor for future mobility. Properly perceiving the environment of the vehicles is essential for a safe driving, which requires computing accurate geometric and semantic information in real-time. In this paper, we challenge state-of-the-art computer vision algorithms for building a perception system for autonomous driving. An inherent drawback in the computation of visual semantics is the trade-off between accuracy and computational cost. We propose to circumvent this problem by following an offline-online strategy. During the offline stage dense 3D semantic maps are created. In the online stage the current driving area is recognized in the maps via a re-localization process, which allows to retrieve the pre-computed accurate semantics and 3D geometry in realtime. Then, detecting the dynamic obstacles we obtain a rich understanding of the current scene. We evaluate quantitatively our proposal in the KITTI dataset and discuss the related open challenges for the computer vision community.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {RRG2015,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {German Ros and Sebastian Ramos and Manuel Granados and Amir Bakhtiary and David Vazquez and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Vision-based Offline-Online Perception Paradigm for Autonomous Driving},<br> &nbsp;&nbsp; booktitle = {IEEE Winter Conference on Applications of Computer Vision WACV2015}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2015}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/rrg2015.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2015-01-01"><div class="media"> <a href=".publication-detailGVX2015" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_GVX2015"><img class="img-responsive" src="./Pubs/GVX2015/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2015</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/gvv2015.pdf">MULTIVIEW RANDOM FOREST OF LOCAL EXPERTS COMBINING RGB AND LIDAR DATA FOR PEDESTRIAN DETECTION</a></h3> <h4>IEEE INTELLIGENT VEHICLES SYMPOSIUM IV2015 (<b>IV</b>), 2015</h4> <span class="publication_description">Despite recent significant advances, pedestrian detection continues to be an extremely challenging problem in real scenarios. In order to develop a detector that successfully operates under these cond...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_GVX2015"></span> <span class="publication_authors">Alejandro Gonzalez Alzate, Gabriel Villalonga, Jiaolong Xu, David Vazquez, Jaume Amores and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailGVX2015 publication-detail"> <div class="image_work" id="image_work_GVX2015"><img class="img-responsive" src="./Pubs/GVX2015/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">MULTIVIEW RANDOM FOREST OF LOCAL EXPERTS COMBINING RGB AND LIDAR DATA FOR PEDESTRIAN DETECTION</h3> <span class="publication_authors">Alejandro Gonzalez Alzate, Gabriel Villalonga, Jiaolong Xu, David Vazquez, Jaume Amores and  Antonio M. Lopez</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">Despite recent significant advances, pedestrian detection continues to be an extremely challenging problem in real scenarios. In order to develop a detector that successfully operates under these conditions, it becomes critical to leverage upon multiple cues, multiple imaging modalities and a strong multi-view classifier that accounts for different pedestrian views and poses. In this paper we provide an extensive evaluation that gives insight into how each of these aspects (multi-cue, multimodality and strong multi-view classifier) affect performance both individually and when integrated together. In the multimodality component we explore the fusion of RGB and depth maps obtained by high-definition LIDAR, a type of modality that is only recently starting to receive attention. As our analysis reveals, although all the aforementioned aspects significantly help in improving the performance, the fusion of visible spectrum and depth information allows to boost the accuracy by a much larger margin. The resulting detector not only ranks among the top best performers in the challenging KITTI benchmark, but it is built upon very simple blocks that are easy to implement and computationally efficient. These simple blocks can be easily replaced with more sophisticated ones recently proposed, such as the use of convolutional neural networks for feature representation, to further improve the accuracy.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {GVX2015,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Alejandro Gonzalez Alzate and Gabriel Villalonga and Jiaolong Xu and David Vazquez and Jaume Amores and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Multiview Random Forest of Local Experts Combining RGB and LIDAR data for Pedestrian Detection},<br> &nbsp;&nbsp; booktitle = {IEEE Intelligent Vehicles Symposium IV2015}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2015}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/gvv2015.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2015-01-01"><div class="media"> <a href=".publication-detailGVR2015" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_GVR2015"><img class="img-responsive" src="./Pubs/GVR2015/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2015</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/GVR2015.pdf">3D-GUIDED MULTISCALE SLIDING WINDOW FOR PEDESTRIAN DETECTION</a></h3> <h4>PATTERN RECOGNITION AND IMAGE ANALYSIS, PROCEEDINGS OF 7TH IBERIAN CONFERENCE , IBPRIA 2015 (<b>IBPRIA</b>), 2015</h4> <span class="publication_description">The most relevant modules of a pedestrian detector are the candidate generation and the candidate classification. The former aims at presenting image windows to the latter so that they are classified ...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_GVR2015"></span> <span class="publication_authors">Alejandro Gonzalez Alzate, Gabriel Villalonga, German Ros, David Vazquez and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailGVR2015 publication-detail"> <div class="image_work" id="image_work_GVR2015"><img class="img-responsive" src="./Pubs/GVR2015/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">3D-GUIDED MULTISCALE SLIDING WINDOW FOR PEDESTRIAN DETECTION</h3> <span class="publication_authors">Alejandro Gonzalez Alzate, Gabriel Villalonga, German Ros, David Vazquez and  Antonio M. Lopez</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">The most relevant modules of a pedestrian detector are the candidate generation and the candidate classification. The former aims at presenting image windows to the latter so that they are classified as containing a pedestrian or not. Much attention has being paid to the classification module, while candidate generation has mainly relied on (multiscale) sliding window pyramid. However, candidate generation is critical for achieving real-time. In this paper we assume a context of autonomous driving based on stereo vision. Accordingly, we evaluate the effect of taking into account the 3D information (derived from the stereo) in order to prune the hundred of thousands windows per image generated by classical pyramidal sliding window. For our study we use a multimodal (RGB, disparity) and multi-descriptor (HOG, LBP, HOG+LBP) holistic ensemble based on linear SVM. Evaluation on data from the challenging KITTI benchmark suite shows the effectiveness of using 3D information to dramatically reduce the number of candidate windows, even improving the overall pedestrian detection accuracy.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {GVR2015,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Alejandro Gonzalez Alzate and Gabriel Villalonga and German Ros and David Vazquez and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {3D-Guided Multiscale Sliding Window for Pedestrian Detection},<br> &nbsp;&nbsp; booktitle = {Pattern Recognition and Image Analysis, Proceedings of 7th Iberian Conference , ibPRIA 2015}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2015}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/GVR2015.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;DEMONSTRATIONS&quot;]" data-date-publication="2015-01-01"><div class="media"> <a href=".publication-detailCSM2015" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_CSM2015"><img class="img-responsive" src="./Pubs/CSM2015/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2015</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/CSM2015.pdf">GPU-BASED PEDESTRIAN DETECTION FOR AUTONOMOUS DRIVING</a></h3> <h4>PROGRAMMING AND TUNNING MASSIVE PARALLEL SYSTEMS (<b>PUMPS</b>), 2015</h4> <span class="publication_description">Pedestrian detection for autonomous driving has gained a lot of prominence during the last few years. Besides the fact that it is one of the hardest tasks within computer vision, it involves huge comp...</span> </div><hr style="margin:8px auto"> <span class="label label-warning">Demonstrations</span><span class="label selected" id="selected_CSM2015"></span> <span class="publication_authors">Victor Campmany, Sergio Silva, Juan Carlos Moure, Antoni Espinosa, David Vazquez and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailCSM2015 publication-detail"> <div class="image_work" id="image_work_CSM2015"><img class="img-responsive" src="./Pubs/CSM2015/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">GPU-BASED PEDESTRIAN DETECTION FOR AUTONOMOUS DRIVING</h3> <span class="publication_authors">Victor Campmany, Sergio Silva, Juan Carlos Moure, Antoni Espinosa, David Vazquez and  Antonio M. Lopez</span> <span class="label label-warning">Demonstrations</span> <span class="label selected">Selected</span> <p class="project_desc">Pedestrian detection for autonomous driving has gained a lot of prominence during the last few years. Besides the fact that it is one of the hardest tasks within computer vision, it involves huge computational costs. The real-time constraints in the field are tight, and regular processors are not able to handle the workload obtaining an acceptable ratio of frames per second (fps). Moreover, multiple cameras are required to obtain accurate results, so the need to speed up the process is even higher. Taking the work in [1] as our baseline, we propose a CUDA implementation of a pedestrian detection system. Further, we introduce significant algorithmic adjustments and optimizations to adapt the problem to the GPU architecture. The aim is to provide a system capable of running in real-time obtaining reliable results.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {CSM2015,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Victor Campmany and Sergio Silva and Juan Carlos Moure and Antoni Espinosa and David Vazquez and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {GPU-based pedestrian detection for autonomous driving},<br> &nbsp;&nbsp; booktitle = {Programming and Tunning Massive Parallel Systems}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2015}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/CSM2015.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;DEMONSTRATIONS&quot;]" data-date-publication="2015-01-01"><div class="media"> <a href=".publication-detailSCS2015" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_SCS2015"><img class="img-responsive" src="./Pubs/SCS2015/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2015</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/SCS2015.pdf">AUTONOMOUS GPU-BASED DRIVING</a></h3> <h4>PROGRAMMING AND TUNNING MASSIVE PARALLEL SYSTEMS (<b>PUMPS</b>), 2015</h4> <span class="publication_description">Human factors cause most driving accidents; this is why nowadays is common to hear about autonomous driving as an alternative. Autonomous driving will not only increase safety, but also will develop a...</span> </div><hr style="margin:8px auto"> <span class="label label-warning">Demonstrations</span><span class="label selected" id="selected_SCS2015"></span> <span class="publication_authors">Sergio Silva, Victor Campmany, Laura Sellart, Juan Carlos Moure, Antoni Espinosa, David Vazquez and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailSCS2015 publication-detail"> <div class="image_work" id="image_work_SCS2015"><img class="img-responsive" src="./Pubs/SCS2015/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">AUTONOMOUS GPU-BASED DRIVING</h3> <span class="publication_authors">Sergio Silva, Victor Campmany, Laura Sellart, Juan Carlos Moure, Antoni Espinosa, David Vazquez and  Antonio M. Lopez</span> <span class="label label-warning">Demonstrations</span> <span class="label selected">Selected</span> <p class="project_desc">Human factors cause most driving accidents; this is why nowadays is common to hear about autonomous driving as an alternative. Autonomous driving will not only increase safety, but also will develop a system of cooperative self-driving cars that will reduce pollution and congestion. Furthermore, it will provide more freedom to handicapped people, elderly or kids.

                  Autonomous Driving requires perceiving and understanding the vehicle environment (e.g., road, traffic signs, pedestrians, vehicles) using sensors (e.g., cameras, lidars, sonars, and radars), selflocalization (requiring GPS, inertial sensors and visual localization in precise maps), controlling the vehicle and planning the routes. These algorithms require high computation capability, and thanks to NVIDIA GPU acceleration this starts to become feasible.

                  NVIDIA® is developing a new platform for boosting the Autonomous Driving capabilities that is able of managing the vehicle via CAN-Bus: the Drive™ PX. It has 8 ARM cores with dual accelerated Tegra® X1 chips. It has 12 synchronized camera inputs for 360º vehicle perception, 4G and Wi-Fi capabilities allowing vehicle communications and GPS and inertial sensors inputs for self-localization.

                  Our research group has been selected for testing Drive™ PX. Accordingly, we are developing a Drive™ PX based autonomous car. Currently, we are porting our previous CPU based algorithms (e.g., Lane Departure Warning, Collision Warning, Automatic Cruise Control, Pedestrian Protection, or Semantic Segmentation) for running in the GPU.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {SCS2015,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Sergio Silva and Victor Campmany and Laura Sellart and Juan Carlos Moure and Antoni Espinosa and David Vazquez and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Autonomous GPU-based Driving},<br> &nbsp;&nbsp; booktitle = {Programming and Tunning Massive Parallel Systems}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2015}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/SCS2015.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;JOURNAL PAPERS&quot;]" data-date-publication="2014-01-01"><div class="media"> <a href=".publication-detailXRV2014b" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_XRV2014b"><img class="img-responsive" src="./Pubs/XRV2014b/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2014</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/XRV2014b.pdf">DOMAIN ADAPTATION OF DEFORMABLE PART-BASED MODELS</a></h3> <h4>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (<b>TPAMI</b>), 36(12), PP. 2367-2380, 2014</h4> <span class="publication_description">The accuracy of object classifiers can significantly drop when the training data (source domain) and the application scenario (target domain) have inherent differences. Therefore, adapting the classif...</span> </div><hr style="margin:8px auto"> <span class="label label-success">Journal papers</span><span class="label selected" id="selected_XRV2014b">Selected</span> <span class="publication_authors">Jiaolong Xu, Sebastian Ramos, David Vazquez and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailXRV2014b publication-detail"> <div class="image_work" id="image_work_XRV2014b"><img class="img-responsive" src="./Pubs/XRV2014b/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">DOMAIN ADAPTATION OF DEFORMABLE PART-BASED MODELS</h3> <span class="publication_authors">Jiaolong Xu, Sebastian Ramos, David Vazquez and  Antonio M. Lopez</span> <span class="label label-success">Journal papers</span> <span class="label selected">Selected</span> <p class="project_desc">The accuracy of object classifiers can significantly drop when the training data (source domain) and the application scenario (target domain) have inherent differences. Therefore, adapting the classifiers to the scenario in which they must operate is of paramount importance. We present novel domain adaptation (DA) methods for object detection. As proof of concept, we focus on adapting the state-of-the-art deformable part-based model (DPM) for pedestrian detection. We introduce an adaptive structural SVM (A-SSVM) that adapts a pre-learned classifier between different domains. By taking into account the inherent structure in feature space (e.g., the parts in a DPM), we propose a structure-aware A-SSVM (SA-SSVM). Neither A-SSVM nor SA-SSVM needs to revisit the source-domain training data to perform the adaptation. Rather, a low number of target-domain training examples (e.g., pedestrians) are used. To address the scenario where there are no target-domain annotated samples, we propose a self-adaptive DPM based on a self-paced learning (SPL) strategy and a Gaussian Process Regression (GPR). Two types of adaptation tasks are assessed: from both synthetic pedestrians and general persons (PASCAL VOC) to pedestrians imaged from an on-board camera. Results show that our proposals avoid accuracy drops as high as 15 points when comparing adapted and non-adapted detectors.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@ARTICLE {XRV2014b,<br> &nbsp;&nbsp; author &nbsp;= {Jiaolong Xu and Sebastian Ramos and David Vazquez and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Domain Adaptation of Deformable Part-Based Models},<br> &nbsp;&nbsp; journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp; = {2014}, <br> &nbsp;&nbsp; volume = {36}, <br> &nbsp;&nbsp; issue &nbsp;&nbsp;&nbsp; = {12}, <br> &nbsp;&nbsp; pages &nbsp;&nbsp; = {2367-2380}  <br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/XRV2014b.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;JOURNAL PAPERS&quot;]" data-date-publication="2014-01-01"><div class="media"> <a href=".publication-detailVML2014" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_VML2014"><img class="img-responsive" src="./Pubs/VML2014/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2014</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/vml2013.pdf">VIRTUAL AND REAL WORLD ADAPTATION FOR PEDESTRIAN DETECTION</a></h3> <h4>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (<b>TPAMI</b>), 36(4), PP. 797-809, 2014</h4> <span class="publication_description">Pedestrian detection is of paramount interest for many applications. Most promising detectors rely on discriminatively learnt classifiers, i.e., trained with annotated samples. However, the annotation...</span> </div><hr style="margin:8px auto"> <span class="label label-success">Journal papers</span><span class="label selected" id="selected_VML2014">Selected</span> <span class="publication_authors">David Vazquez, Javier Marin, Antonio M. Lopez, Daniel Ponsa and  David Geronimo</span> </div><div class="mfp-hide mfp-with-anim publication-detailVML2014 publication-detail"> <div class="image_work" id="image_work_VML2014"><img class="img-responsive" src="./Pubs/VML2014/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">VIRTUAL AND REAL WORLD ADAPTATION FOR PEDESTRIAN DETECTION</h3> <span class="publication_authors">David Vazquez, Javier Marin, Antonio M. Lopez, Daniel Ponsa and  David Geronimo</span> <span class="label label-success">Journal papers</span> <span class="label selected">Selected</span> <p class="project_desc">Pedestrian detection is of paramount interest for many applications. Most promising detectors rely on discriminatively learnt classifiers, i.e., trained with annotated samples. However, the annotation step is a human intensive and subjective task worth to be minimized. By using virtual worlds we can automatically obtain precise and rich annotations. Thus, we face the question: can a pedestrian appearance model learnt in realistic virtual worlds work successfully for pedestrian detection in realworld images?. Conducted experiments show that virtual-world based training can provide excellent testing accuracy in real world, but it can also suffer the dataset shift problem as real-world based training does. Accordingly, we have designed a domain adaptation framework, V-AYLA, in which we have tested different techniques to collect a few pedestrian samples from the target domain (real world) and combine them with the many examples of the source domain (virtual world) in order to train a domain adapted pedestrian classifier that will operate in the target domain. V-AYLA reports the same detection accuracy than when training with many human-provided pedestrian annotations and testing with real-world images of the same domain. To the best of our knowledge, this is the first work demonstrating adaptation of virtual and real worlds for developing an object detector.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@ARTICLE {VML2014,<br> &nbsp;&nbsp; author &nbsp;= {David Vazquez and Javier Marin and Antonio Lopez and Daniel Ponsa and David Geronimo}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Virtual and Real World Adaptation for Pedestrian Detection},<br> &nbsp;&nbsp; journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp; = {2014}, <br> &nbsp;&nbsp; volume = {36}, <br> &nbsp;&nbsp; issue &nbsp;&nbsp;&nbsp; = {4}, <br> &nbsp;&nbsp; pages &nbsp;&nbsp; = {797-809}  <br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/vml2013.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;JOURNAL PAPERS&quot;]" data-date-publication="2014-01-01"><div class="media"> <a href=".publication-detailMVL2014" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_MVL2014"><img class="img-responsive" src="./Pubs/MVL2014/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2014</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/mvl2013a.pdf">OCCLUSION HANDLING VIA RANDOM SUBSPACE CLASSIFIERS FOR HUMAN DETECTION</a></h3> <h4>IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS (PART B) (<b>TSMCB</b>), 44(3), PP. 342-354, 2014</h4> <span class="publication_description">This paper describes a general method to address partial occlusions for human detection in still images. The Random Subspace Method (RSM) is chosen for building a classifier ensemble robust against pa...</span> </div><hr style="margin:8px auto"> <span class="label label-success">Journal papers</span><span class="label selected" id="selected_MVL2014">Selected</span> <span class="publication_authors">Javier Marin, David Vazquez, Antonio M. Lopez, Jaume Amores and  Ludmila I. Kuncheva</span> </div><div class="mfp-hide mfp-with-anim publication-detailMVL2014 publication-detail"> <div class="image_work" id="image_work_MVL2014"><img class="img-responsive" src="./Pubs/MVL2014/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">OCCLUSION HANDLING VIA RANDOM SUBSPACE CLASSIFIERS FOR HUMAN DETECTION</h3> <span class="publication_authors">Javier Marin, David Vazquez, Antonio M. Lopez, Jaume Amores and  Ludmila I. Kuncheva</span> <span class="label label-success">Journal papers</span> <span class="label selected">Selected</span> <p class="project_desc">This paper describes a general method to address partial occlusions for human detection in still images. The Random Subspace Method (RSM) is chosen for building a classifier ensemble robust against partial occlusions. The component classifiers are chosen on the basis of their individual and combined performance. The main contribution of this work lies in our approach’s capability to improve the detection rate when partial occlusions are present without compromising the detection performance on non occluded data. In contrast to many recent approaches, we propose a method which does not require manual labelling of body parts, defining any semantic spatial components, or using additional data coming from motion or stereo. Moreover, the method can be easily extended to other object classes. The experiments are performed on three large datasets: the INRIA person dataset, the Daimler Multicue dataset, and a new challenging dataset, called PobleSec, in which a considerable number of targets are partially occluded. The different approaches are evaluated at the classification and detection levels for both partially occluded and non-occluded data. The experimental results show that our detector outperforms state-of-the-art approaches in the presence of partial occlusions, while offering performance and reliability similar to those of the holistic approach on non-occluded data. The datasets used in our experiments have been made publicly available for benchmarking purposes</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@ARTICLE {MVL2014,<br> &nbsp;&nbsp; author &nbsp;= {Javier Marin and David Vazquez and Antonio Lopez and Jaume Amores and Ludmila I. Kuncheva}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Occlusion handling via random subspace classifiers for human detection},<br> &nbsp;&nbsp; journal = {IEEE Transactions on Systems, Man, and Cybernetics (Part B)}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp; = {2014}, <br> &nbsp;&nbsp; volume = {44}, <br> &nbsp;&nbsp; issue &nbsp;&nbsp;&nbsp; = {3}, <br> &nbsp;&nbsp; pages &nbsp;&nbsp; = {342-354}  <br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/mvl2013a.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;JOURNAL PAPERS&quot;]" data-date-publication="2014-01-01"><div class="media"> <a href=".publication-detailXVL2014" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_XVL2014"><img class="img-responsive" src="./Pubs/XVL2014/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2014</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/xvl2014.pdf">LEARNING A PART-BASED PEDESTRIAN DETECTOR IN VIRTUAL WORLD</a></h3> <h4>IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS (<b>TITS</b>), 15(5), PP. 2121-2131, 2014</h4> <span class="publication_description">Detecting pedestrians with on-board vision systems is of paramount interest for assisting drivers to prevent vehicle-to-pedestrian accidents. The core of a pedestrian detector is its classification mo...</span> </div><hr style="margin:8px auto"> <span class="label label-success">Journal papers</span><span class="label selected" id="selected_XVL2014">Selected</span> <span class="publication_authors">Jiaolong Xu, David Vazquez, Antonio M. Lopez, Javier Marin and  Daniel Ponsa</span> </div><div class="mfp-hide mfp-with-anim publication-detailXVL2014 publication-detail"> <div class="image_work" id="image_work_XVL2014"><img class="img-responsive" src="./Pubs/XVL2014/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">LEARNING A PART-BASED PEDESTRIAN DETECTOR IN VIRTUAL WORLD</h3> <span class="publication_authors">Jiaolong Xu, David Vazquez, Antonio M. Lopez, Javier Marin and  Daniel Ponsa</span> <span class="label label-success">Journal papers</span> <span class="label selected">Selected</span> <p class="project_desc">Detecting pedestrians with on-board vision systems is of paramount interest for assisting drivers to prevent vehicle-to-pedestrian accidents. The core of a pedestrian detector is its classification module, which aims at deciding if a given image window contains a pedestrian. Given the difficulty of this task, many classifiers have been proposed during the last fifteen years. Among them, the so-called (deformable) part-based classifiers including multi-view modeling are usually top ranked in accuracy. Training such classifiers is not trivial since a proper aspect clustering and spatial part alignment of the pedestrian training samples are crucial for obtaining an accurate classifier. In this paper, first we perform automatic aspect clustering and part alignment by using virtual-world pedestrians, i.e., human annotations are not required. Second, we use a mixture-of-parts approach that allows part sharing among different aspects. Third, these proposals are integrated in a learning framework which also allows to incorporate real-world training data to perform domain adaptation between virtual- and real-world cameras. Overall, the obtained results on four popular on-board datasets show that our proposal clearly outperforms the state-of-the-art deformable part-based detector known as latent SVM.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@ARTICLE {XVL2014,<br> &nbsp;&nbsp; author &nbsp;= {Jiaolong Xu and David Vazquez and Antonio Lopez and Javier Marin and Daniel Ponsa}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Learning a Part-based Pedestrian Detector in Virtual World},<br> &nbsp;&nbsp; journal = {IEEE Transactions on Intelligent Transportation Systems}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp; = {2014}, <br> &nbsp;&nbsp; volume = {15}, <br> &nbsp;&nbsp; issue &nbsp;&nbsp;&nbsp; = {5}, <br> &nbsp;&nbsp; pages &nbsp;&nbsp; = {2121-2131}  <br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/xvl2014.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2014-01-01"><div class="media"> <a href=".publication-detailXRV2014a" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_XRV2014a"><img class="img-responsive" src="./Pubs/XRV2014a/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2014</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/xrv2014.pdf">COST-SENSITIVE STRUCTURED SVM FOR MULTI-CATEGORY DOMAIN ADAPTATION</a></h3> <h4>22ND INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (<b>ICPR</b>), 2014</h4> <span class="publication_description">Domain adaptation addresses the problem of accuracy drop that a classifier may suffer when the training data (source domain) and the testing data (target domain) are drawn from different distributions...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_XRV2014a"></span> <span class="publication_authors">Jiaolong Xu, Sebastian Ramos,David Vazquez and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailXRV2014a publication-detail"> <div class="image_work" id="image_work_XRV2014a"><img class="img-responsive" src="./Pubs/XRV2014a/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">COST-SENSITIVE STRUCTURED SVM FOR MULTI-CATEGORY DOMAIN ADAPTATION</h3> <span class="publication_authors">Jiaolong Xu, Sebastian Ramos,David Vazquez and  Antonio M. Lopez</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">Domain adaptation addresses the problem of accuracy drop that a classifier may suffer when the training data (source domain) and the testing data (target domain) are drawn from different distributions. In this work, we focus on domain adaptation for structured SVM (SSVM). We propose a cost-sensitive domain adaptation method for SSVM, namely COSS-SSVM. In particular, during the re-training of an adapted classifier based on target and source data, the idea that we explore consists in introducing a non-zero cost even for correctly classified source domain samples. Eventually, we aim to learn a more targetoriented classifier by not rewarding (zero loss) properly classified source-domain training samples. We assess the effectiveness of COSS-SSVM on multi-category object recognition.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {XRV2014a,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Jiaolong Xu and Sebastian Ramos andDavid Vazquez and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Cost-sensitive Structured SVM for Multi-category Domain Adaptation},<br> &nbsp;&nbsp; booktitle = {22nd International Conference on Pattern Recognition}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2014}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/xrv2014.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2014-01-01"><div class="media"> <a href=".publication-detailxrv2014c" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_xrv2014c"><img class="img-responsive" src="./Pubs/xrv2014c/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2014</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/xrv2014c.pdf">INCREMENTAL DOMAIN ADAPTATION OF DEFORMABLE PART-BASED MODELS</a></h3> <h4>25TH BRITISH MACHINE VISION CONFERENCE (<b>BMVC</b>), 2014</h4> <span class="publication_description">Nowadays, classifiers play a core role in many computer vision tasks. The underlying assumption for learning classifiers is that the training set and the deployment environment (testing) follow the sa...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_xrv2014c"></span> <span class="publication_authors">Jiaolong Xu, Sebastian Ramos, David Vazquez and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailxrv2014c publication-detail"> <div class="image_work" id="image_work_xrv2014c"><img class="img-responsive" src="./Pubs/xrv2014c/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">INCREMENTAL DOMAIN ADAPTATION OF DEFORMABLE PART-BASED MODELS</h3> <span class="publication_authors">Jiaolong Xu, Sebastian Ramos, David Vazquez and  Antonio M. Lopez</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">Nowadays, classifiers play a core role in many computer vision tasks. The underlying assumption for learning classifiers is that the training set and the deployment environment (testing) follow the same probability distribution regarding the features used by the classifiers. However, in practice, there are different reasons that can break this constancy assumption. Accordingly, reusing existing classifiers by adapting them from the previous training environment (source domain) to the new testing one (target domain)
                  is an approach with increasing acceptance in the computer vision community. In this paper we focus on the domain adaptation of deformable part-based models (DPMs) for object detection. In particular, we focus on a relatively unexplored scenario, i.e. incremental domain adaptation for object detection assuming weak-labeling. Therefore, our algorithm is ready to improve existing source-oriented DPM-based detectors as soon as a little amount of labeled target-domain training data is available, and keeps improving as more of such data arrives in a continuous fashion. For achieving this, we follow a multiple
                  instance learning (MIL) paradigm that operates in an incremental per-image basis. As proof of concept, we address the challenging scenario of adapting a DPM-based pedestrian detector trained with synthetic pedestrians to operate in real-world scenarios. The obtained results show that our incremental adaptive models obtain equally good accuracy results as the batch learned models, while being more flexible for handling continuously arriving target-domain data.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {xrv2014c,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Jiaolong Xu and Sebastian Ramos and David Vazquez and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Incremental Domain Adaptation of Deformable Part-based Models},<br> &nbsp;&nbsp; booktitle = {25th British Machine Vision Conference}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2014}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/xrv2014c.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;DEMONSTRATIONS&quot;]" data-date-publication="2014-01-01"><div class="media"> <a href=".publication-detailVRR2014" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_VRR2014"><img class="img-responsive" src="./Pubs/VRR2014/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2014</span></div><div class="media-body"> <h3><a class="ext_link" href="">3D PEDESTRIAN DETECTION VIA RANDOM FOREST</a></h3> <h4>EUROPEAN CONFERENCE ON COMPUTER VISION (<b>ECCV-DEMO</b>), 2014</h4> <span class="publication_description">Our demo focuses on showing the extraordinary performance of our novel 3D pedestrian detector along with its simplicity and real-time capabilities. This detector has been designed for autonomous drivi...</span> </div><hr style="margin:8px auto"> <span class="label label-warning">Demonstrations</span><span class="label selected" id="selected_VRR2014"></span> <span class="publication_authors">G. Villalonga, Sebastian Ramos, German Ros, David Vazquez and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailVRR2014 publication-detail"> <div class="image_work" id="image_work_VRR2014"><img class="img-responsive" src="./Pubs/VRR2014/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">3D PEDESTRIAN DETECTION VIA RANDOM FOREST</h3> <span class="publication_authors">G. Villalonga, Sebastian Ramos, German Ros, David Vazquez and  Antonio M. Lopez</span> <span class="label label-warning">Demonstrations</span> <span class="label selected">Selected</span> <p class="project_desc">Our demo focuses on showing the extraordinary performance of our novel 3D pedestrian detector along with its simplicity and real-time capabilities. This detector has been designed for autonomous driving applications, but it can also be applied in other scenarios that cover both outdoor and indoor applications.
                  Our pedestrian detector is based on the combination of a random forest classifier with HOG-LBP features and the inclusion of a preprocessing stage based on 3D scene information in order to precisely determinate the image regions where the detector should search for pedestrians. This approach ends up in a high accurate system that runs real-time as it is required by many computer vision and robotics applications.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {VRR2014,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {G. Villalonga and Sebastian Ramos and German Ros and David Vazquez and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {3d Pedestrian Detection via Random Forest},<br> &nbsp;&nbsp; booktitle = {European Conference on Computer Vision}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2014}<br>}</div><a class="ext_link" href=""><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2013-01-01"><div class="media"> <a href=".publication-detailSRV2013" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_SRV2013"><img class="img-responsive" src="./Pubs/SRV2013/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2013</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/srv2013.pdf">ADAPTING PEDESTRIAN DETECTION FROM SYNTHETIC TO FAR INFRARED IMAGES</a></h3> <h4>ICCV WORKSHOP ON VISUAL DOMAIN ADAPTATION AND DATASET BIAS (<b>ICCVW-VISDA</b>), 2013</h4> <span class="publication_description">We present different techniques to adapt a pedestrian classifier trained with synthetic images and the corresponding automatically generated annotations to operate with far infrared (FIR) images. The ...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_SRV2013"></span> <span class="publication_authors">Yainuvis Socarras, Sebastian Ramos, David Vazquez, Antonio M. Lopez and  Theo Gevers</span> </div><div class="mfp-hide mfp-with-anim publication-detailSRV2013 publication-detail"> <div class="image_work" id="image_work_SRV2013"><img class="img-responsive" src="./Pubs/SRV2013/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">ADAPTING PEDESTRIAN DETECTION FROM SYNTHETIC TO FAR INFRARED IMAGES</h3> <span class="publication_authors">Yainuvis Socarras, Sebastian Ramos, David Vazquez, Antonio M. Lopez and  Theo Gevers</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">We present different techniques to adapt a pedestrian classifier trained with synthetic images and the corresponding automatically generated annotations to operate with far infrared (FIR) images. The information contained in this kind of images allow us to develop a robust pedestrian detector invariant to extreme illumination changes.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {SRV2013,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Yainuvis Socarras and Sebastian Ramos and David Vazquez and Antonio Lopez and Theo Gevers}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Adapting Pedestrian Detection from Synthetic to Far Infrared Images},<br> &nbsp;&nbsp; booktitle = {ICCV Workshop on Visual Domain Adaptation and Dataset Bias}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2013}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/srv2013.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;BOOK&quot;]" data-date-publication="2013-01-01"><div class="media"> <a href=".publication-detailBeV2013" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_BeV2013"><img class="img-responsive" src="./Pubs/BeV2013/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2013</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/bv2013.pdf">COMPUTER VISION TRENDS AND CHALLENGES</a></h3> <h4>BOOK: COMPUTER VISION TRENDS AND CHALLENGES, 2013</h4> <span class="publication_description">This book contains the papers presented at the Eighth CVC Workshop on Computer Vision Trends and Challenges (CVCR&amp;D'2013). The workshop was held at the Computer Vision Center (Universitat Autònoma de ...</span> </div><hr style="margin:8px auto"> <span class="label label-info">Book</span><span class="label selected" id="selected_BeV2013"></span> <span class="publication_authors">Jorge Bernal, David Vazquez (eds)</span> </div><div class="mfp-hide mfp-with-anim publication-detailBeV2013 publication-detail"> <div class="image_work" id="image_work_BeV2013"><img class="img-responsive" src="./Pubs/BeV2013/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">COMPUTER VISION TRENDS AND CHALLENGES</h3> <span class="publication_authors">Jorge Bernal, David Vazquez (eds)</span> <span class="label label-info">Book</span> <span class="label selected">Selected</span> <p class="project_desc">This book contains the papers presented at the Eighth CVC Workshop on Computer Vision Trends and Challenges (CVCR&amp;D'2013). The workshop was held at the Computer Vision Center (Universitat Autònoma de Barcelona), the October 25th, 2013. The CVC workshops provide an excellent opportunity for young researchers and project engineers to share new ideas and knowledge about the progress of their work, and also, to discuss about challenges and future perspectives. In addition, the workshop is the welcome event for new people that recently have joined the institute.

                  The program of CVCR&amp;D is organized in a single-track single-day workshop. It comprises several sessions dedicated to specific topics. For each session, a doctor working on the topic introduces the general research lines. The PhD students expose their specific research. A poster session will be held for open questions. Session topics cover the current research lines and development projects of the CVC: Medical Imaging, Medical Imaging, Color &amp; Texture Analysis, Object Recognition, Image Sequence Evaluation, Advanced Driver Assistance Systems, Machine Vision, Document Analysis, Pattern Recognition and Applications. We want to thank all paper authors and Program Committee members. Their contribution shows that the CVC has a dynamic, active, and promising scientific community.

                  We hope you all enjoy this Eighth workshop and we are looking forward to meeting you and new people next year in the Ninth CVCR&amp;D.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@BOOK {BeV2013,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Jorge Bernal and David Vazquez (eds)}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Computer vision Trends and Challenges},<br> &nbsp;&nbsp; booktitle = {Computer vision Trends and Challenges}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2013}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/bv2013.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2013-01-01"><div class="media"> <a href=".publication-detailXRH2013" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_XRH2013"><img class="img-responsive" src="./Pubs/XRH2013/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2013</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/xrh2013.pdf">MULTI-TASK BILINEAR CLASSIFIERS FOR VISUAL DOMAIN ADAPTATION</a></h3> <h4>ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS WORKSHOP (<b>NIPSW</b>), 2013</h4> <span class="publication_description">We propose a method that aims to lessen the significant accuracy degradation
                  that a discriminative classifier can suffer when it is trained in a specific domain (source domain) and applied in a diffe...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_XRH2013"></span> <span class="publication_authors">Jiaolong Xu, Sebastian Ramos, Xu Hu, David Vazquez and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailXRH2013 publication-detail"> <div class="image_work" id="image_work_XRH2013"><img class="img-responsive" src="./Pubs/XRH2013/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">MULTI-TASK BILINEAR CLASSIFIERS FOR VISUAL DOMAIN ADAPTATION</h3> <span class="publication_authors">Jiaolong Xu, Sebastian Ramos, Xu Hu, David Vazquez and  Antonio M. Lopez</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">We propose a method that aims to lessen the significant accuracy degradation
                  that a discriminative classifier can suffer when it is trained in a specific domain (source domain) and applied in a different one (target domain). The principal reason for this degradation is the discrepancies in the distribution of the features that feed the classifier in different domains. Therefore, we propose a domain adaptation method that maps the features from the different domains into a common subspace and learns a discriminative domain-invariant classifier within it. Our algorithm combines bilinear classifiers and multi-task learning for domain adaptation.
                  The bilinear classifier encodes the feature transformation and classification
                  parameters by a matrix decomposition. In this way, specific feature transformations for multiple domains and a shared classifier are jointly learned in a multi-task learning framework. Focusing on domain adaptation for visual object detection, we apply this method to the state-of-the-art deformable part-based model for cross domain pedestrian detection. Experimental results show that our method significantly avoids the domain drift and improves the accuracy when compared to several baselines.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {XRH2013,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Jiaolong Xu and Sebastian Ramos and Xu Hu and David Vazquez and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Multi-task Bilinear Classifiers for Visual Domain Adaptation},<br> &nbsp;&nbsp; booktitle = {Advances in Neural Information Processing Systems Workshop}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2013}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/xrh2013.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2013-01-01"><div class="media"> <a href=".publication-detailMVL2013" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_MVL2013"><img class="img-responsive" src="./Pubs/MVL2013/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2013</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/mvl2013b.pdf">RANDOM FORESTS OF LOCAL EXPERTS FOR PEDESTRIAN DETECTION</a></h3> <h4>15TH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION (<b>ICCV</b>), 2013</h4> <span class="publication_description">Pedestrian detection is one of the most challenging tasks in computer vision, and has received a lot of attention in the last years. Recently, some authors have shown the advantages of using combinati...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_MVL2013"></span> <span class="publication_authors">Javier Marin, David Vazquez, Antonio M. Lopez, Jaume Amores and  Bastian Leibe</span> </div><div class="mfp-hide mfp-with-anim publication-detailMVL2013 publication-detail"> <div class="image_work" id="image_work_MVL2013"><img class="img-responsive" src="./Pubs/MVL2013/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">RANDOM FORESTS OF LOCAL EXPERTS FOR PEDESTRIAN DETECTION</h3> <span class="publication_authors">Javier Marin, David Vazquez, Antonio M. Lopez, Jaume Amores and  Bastian Leibe</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">Pedestrian detection is one of the most challenging tasks in computer vision, and has received a lot of attention in the last years. Recently, some authors have shown the advantages of using combinations of part/patch-based detectors in order to cope with the large variability of poses and the existence of partial occlusions. In this paper, we propose a pedestrian detection method that efficiently combines multiple local experts by means of a Random Forest ensemble. The proposed method works with rich block-based representations such as HOG and LBP, in such a way that the same features are reused by the multiple local experts, so that no extra computational cost is needed with respect to a holistic method. Furthermore, we demonstrate how to integrate the proposed approach with a cascaded architecture in order to achieve not only high accuracy but also an acceptable efficiency. In particular, the resulting detector operates at five frames per second using a laptop machine. We tested the proposed method with well-known challenging datasets such as Caltech, ETH, Daimler, and INRIA. The method proposed in this work consistently ranks among the top performers in all the datasets, being either the best method or having a small difference with the best one.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {MVL2013,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Javier Marin and David Vazquez and Antonio Lopez and Jaume Amores and Bastian Leibe}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Random Forests of Local Experts for Pedestrian Detection},<br> &nbsp;&nbsp; booktitle = {15th IEEE International Conference on Computer Vision}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2013}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/mvl2013b.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;BOOK&quot;]" data-date-publication="2013-01-01"><div class="media"> <a href=".publication-detailVaz2013" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_Vaz2013"><img class="img-responsive" src="./Pubs/Vaz2013/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2013</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/v2013.pdf">DOMAIN ADAPTATION OF VIRTUAL AND REAL WORLDS FOR PEDESTRIAN DETECTION</a></h3> <h4>BOOK: PHD THESIS, UNIVERSITAT DE BARCELONA-CVC, 2013</h4> <span class="publication_description">Pedestrian detection is of paramount interest for many applications, e.g. Advanced Driver Assistance Systems, Intelligent Video Surveillance and Multimedia systems. Most promising pedestrian detectors...</span> </div><hr style="margin:8px auto"> <span class="label label-info">Book</span><span class="label selected" id="selected_Vaz2013"></span> <span class="publication_authors">David Vazquez</span> </div><div class="mfp-hide mfp-with-anim publication-detailVaz2013 publication-detail"> <div class="image_work" id="image_work_Vaz2013"><img class="img-responsive" src="./Pubs/Vaz2013/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">DOMAIN ADAPTATION OF VIRTUAL AND REAL WORLDS FOR PEDESTRIAN DETECTION</h3> <span class="publication_authors">David Vazquez</span> <span class="label label-info">Book</span> <span class="label selected">Selected</span> <p class="project_desc">Pedestrian detection is of paramount interest for many applications, e.g. Advanced Driver Assistance Systems, Intelligent Video Surveillance and Multimedia systems. Most promising pedestrian detectors rely on appearance-based classifiers trained with annotated data. However, the required annotation step represents an intensive and subjective task for humans, what makes worth to minimize their intervention in this process by using computational tools like realistic virtual worlds. The reason to use these kind of tools relies in the fact that they allow the automatic generation of precise and rich annotations of visual information. Nevertheless, the use of this kind of data comes with the following question: can a pedestrian appearance model learnt with virtual-world data work successfully for pedestrian detection in real-world scenarios?. To answer this question, we conduct different experiments that suggest a positive answer. However, the pedestrian classifiers trained with virtual-world data can suffer the so called dataset shift problem as real-world based classifiers does. Accordingly, we have designed different domain adaptation techniques to face this problem, all of them integrated in a same framework (V-AYLA). We have explored different methods to train a domain adapted pedestrian classifiers by collecting a few pedestrian samples from the target domain (real world) and combining them with many samples of the source domain (virtual world). The extensive experiments we present show that pedestrian detectors developed within the V-AYLA framework do achieve domain adaptation. Ideally, we would like to adapt our system without any human intervention. Therefore, as a first proof of concept we also propose an unsupervised domain adaptation technique that avoids human intervention during the adaptation process. To the best of our knowledge, this Thesis work is the first demonstrating adaptation of virtual and real worlds for developing an object detector. Last but not least, we also assessed a different strategy to avoid the dataset shift that consists in collecting real-world samples and retrain with them in such a way that no bounding boxes of real-world pedestrians have to be provided. We show that the generated classifier is competitive with respect to the counterpart trained with samples collected by manually annotating pedestrian bounding boxes. The results presented on this Thesis not only end with a proposal for adapting a virtual-world pedestrian detector to the real world, but also it goes further by pointing out a new methodology that would allow the system to adapt to different situations, which we hope will provide the foundations for future research in this unexplored area.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@BOOK {Vaz2013,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {David Vazquez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Domain Adaptation of Virtual and Real Worlds for Pedestrian Detection},<br> &nbsp;&nbsp; booktitle = {PhD Thesis, Universitat de Barcelona-CVC}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2013}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/v2013.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;BOOK CHAPTERS&quot;]" data-date-publication="2013-01-01"><div class="media"> <a href=".publication-detailvlp2013" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_vlp2013"><img class="img-responsive" src="./Pubs/vlp2013/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2013</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/vlp2012c.pdf">INTERACTIVE TRAINING OF HUMAN DETECTORS</a></h3> <h4>BOOK CHAPTER: MULTIMODAL INTERACTION IN IMAGE AND VIDEO APPLICATIONS INTELLIGENT SYSTEMS REFERENCE LIBRARY, 2013</h4> <span class="publication_description">Image based human detection remains as a challenging problem. Most promising detectors rely on classifiers trained with labelled samples. However, labelling is a manual labor intensive step. To overco...</span> </div><hr style="margin:8px auto"> <span class="label label-danger">Book Chapters</span><span class="label selected" id="selected_vlp2013"></span> <span class="publication_authors">David Vazquez, Antonio M. Lopez, Daniel Ponsa and  David Geronimo</span> </div><div class="mfp-hide mfp-with-anim publication-detailvlp2013 publication-detail"> <div class="image_work" id="image_work_vlp2013"><img class="img-responsive" src="./Pubs/vlp2013/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">INTERACTIVE TRAINING OF HUMAN DETECTORS</h3> <span class="publication_authors">David Vazquez, Antonio M. Lopez, Daniel Ponsa and  David Geronimo</span> <span class="label label-danger">Book Chapters</span> <span class="label selected">Selected</span> <p class="project_desc">Image based human detection remains as a challenging problem. Most promising detectors rely on classifiers trained with labelled samples. However, labelling is a manual labor intensive step. To overcome this problem we propose to collect images of pedestrians from a virtual city, i.e., with automatic labels, and train a pedestrian detector with them, which works fine when such virtual-world data are similar to testing one, i.e., real-world pedestrians in urban areas. When testing data is acquired in different conditions than training one, e.g., human detection in personal photo albums, dataset shift appears. In previous work, we cast this problem as one of domain adaptation and solve it with an active learning procedure. In this work, we focus on the same problem but evaluating a different set of faster to compute features, i.e., Haar, EOH and their combination. In particular, we train a classifier with virtual-world data, using such features and Real AdaBoost as learning machine. This classifier is applied to real-world training images. Then, a human oracle interactively corrects the wrong detections, i.e., few miss detections are manually annotated and some false ones are pointed out too. A low amount of manual annotation is fixed as restriction. Real- and virtual-world difficult samples are combined within what we call cool world and we retrain the classifier with this data. Our experiments show that this adapted classifier is equivalent to the one trained with only real-world data but requiring 90% less manual annotations.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INBOOK {vlp2013,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {David Vazquez and Antonio Lopez and Daniel Ponsa and David Geronimo}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Interactive Training of Human Detectors},<br> &nbsp;&nbsp; booktitle = {Multimodal Interaction in Image and Video Applications Intelligent Systems Reference Library}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2013}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/vlp2012c.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2013-01-01"><div class="media"> <a href=".publication-detailxvl2013a" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_xvl2013a"><img class="img-responsive" src="./Pubs/xvl2013a/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2013</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/xvl2013a.pdf">LEARNING A MULTIVIEW PART-BASED MODEL IN VIRTUAL WORLD FOR PEDESTRIAN DETECTION</a></h3> <h4>IEEE INTELLIGENT VEHICLES SYMPOSIUM (<b>IV</b>), 2013</h4> <span class="publication_description">State-of-the-art deformable part-based models based on latent SVM have shown excellent results on human detection. In this paper, we propose to train a multiview deformable part-based model with autom...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_xvl2013a"></span> <span class="publication_authors">Jiaolong Xu, David Vazquez, Antonio M. Lopez, Javier Marin and  Daniel Ponsa</span> </div><div class="mfp-hide mfp-with-anim publication-detailxvl2013a publication-detail"> <div class="image_work" id="image_work_xvl2013a"><img class="img-responsive" src="./Pubs/xvl2013a/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">LEARNING A MULTIVIEW PART-BASED MODEL IN VIRTUAL WORLD FOR PEDESTRIAN DETECTION</h3> <span class="publication_authors">Jiaolong Xu, David Vazquez, Antonio M. Lopez, Javier Marin and  Daniel Ponsa</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">State-of-the-art deformable part-based models based on latent SVM have shown excellent results on human detection. In this paper, we propose to train a multiview deformable part-based model with automatically generated part examples from virtual-world data. The method is efficient as: (i) the part detectors are trained with precisely extracted virtual examples, thus no latent learning is needed, (ii) the multiview pedestrian detector enhances the performance of the pedestrian root model, (iii) a top-down approach is used for part detection which reduces the searching space. We evaluate our model on Daimler and Karlsruhe Pedestrian Benchmarks with publicly available Caltech pedestrian detection evaluation framework and the result outperforms the state-of-the-art latent SVM V4.0, on both average miss rate and speed (our detector is ten times faster).</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {xvl2013a,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Jiaolong Xu and David Vazquez and Antonio Lopez and Javier Marin and Daniel Ponsa}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Learning a Multiview Part-based Model in Virtual World for Pedestrian Detection},<br> &nbsp;&nbsp; booktitle = {IEEE Intelligent Vehicles Symposium}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2013}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/xvl2013a.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2013-01-01"><div class="media"> <a href=".publication-detailVXR2013a" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_VXR2013a"><img class="img-responsive" src="./Pubs/VXR2013a/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2013</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/vxr2013a.pdf">WEAKLY SUPERVISED AUTOMATIC ANNOTATION OF PEDESTRIAN BOUNDING BOXES</a></h3> <h4>CVPR WORKSHOP ON GROUND TRUTH - WHAT IS A GOOD DATASET? (<b>CVPRW</b>), 2013</h4> <span class="publication_description">Among the components of a pedestrian detector, its trained pedestrian classifier is crucial for achieving the desired performance. The initial task of the training process consists in collecting sampl...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_VXR2013a"></span> <span class="publication_authors">David Vazquez, Jiaolong Xu, Sebastian Ramos, Antonio M. Lopez and  Daniel Ponsa</span> </div><div class="mfp-hide mfp-with-anim publication-detailVXR2013a publication-detail"> <div class="image_work" id="image_work_VXR2013a"><img class="img-responsive" src="./Pubs/VXR2013a/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">WEAKLY SUPERVISED AUTOMATIC ANNOTATION OF PEDESTRIAN BOUNDING BOXES</h3> <span class="publication_authors">David Vazquez, Jiaolong Xu, Sebastian Ramos, Antonio M. Lopez and  Daniel Ponsa</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">Among the components of a pedestrian detector, its trained pedestrian classifier is crucial for achieving the desired performance. The initial task of the training process consists in collecting samples of pedestrians and background, which involves tiresome manual annotation of pedestrian bounding boxes (BBs). Thus, recent works have assessed the use of automatically collected samples from photo-realistic virtual worlds. However, learning from virtual-world samples and testing in real-world images may suffer the dataset shift problem. Accordingly, in this paper we assess an strategy to collect samples from the real world and retrain with them, thus avoiding the dataset shift, but in such a way that no BBs of real-world pedestrians have to be provided. In particular, we train a pedestrian classifier based on virtual-world samples (no human annotation required). Then, using such a classifier we collect pedestrian samples from real-world images by detection. After, a human oracle rejects the false detections efficiently (weak annotation). Finally, a new classifier is trained with the accepted detections. We show that this classifier is competitive with respect to the counterpart trained with samples collected by manually annotating hundreds of pedestrian BBs.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {VXR2013a,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {David Vazquez and Jiaolong Xu and Sebastian Ramos and Antonio Lopez and Daniel Ponsa}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Weakly Supervised Automatic Annotation of Pedestrian Bounding Boxes},<br> &nbsp;&nbsp; booktitle = {CVPR Workshop on Ground Truth - What is a good dataset?}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2013}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/vxr2013a.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2013-01-01"><div class="media"> <a href=".publication-detailxvr2013a" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_xvr2013a"><img class="img-responsive" src="./Pubs/xvr2013a/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2013</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/xvr2013a.pdf">ADAPTING A PEDESTRIAN DETECTOR BY BOOSTING LDA EXEMPLAR CLASSIFIERS</a></h3> <h4>CVPR WORKSHOP ON GROUND TRUTH - WHAT IS A GOOD DATASET? (<b>CVPRW</b>), 2013</h4> <span class="publication_description">Training vision-based pedestrian detectors using synthetic datasets (virtual world) is a useful technique to collect automatically the training examples with their pixel-wise ground truth. However, as...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_xvr2013a"></span> <span class="publication_authors">Jiaolong Xu, David Vazquez, Sebastian Ramos, Antonio M. Lopez and  Daniel Ponsa</span> </div><div class="mfp-hide mfp-with-anim publication-detailxvr2013a publication-detail"> <div class="image_work" id="image_work_xvr2013a"><img class="img-responsive" src="./Pubs/xvr2013a/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">ADAPTING A PEDESTRIAN DETECTOR BY BOOSTING LDA EXEMPLAR CLASSIFIERS</h3> <span class="publication_authors">Jiaolong Xu, David Vazquez, Sebastian Ramos, Antonio M. Lopez and  Daniel Ponsa</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">Training vision-based pedestrian detectors using synthetic datasets (virtual world) is a useful technique to collect automatically the training examples with their pixel-wise ground truth. However, as it is often the case, these detectors must operate in real-world images, experiencing a significant drop of their performance. In fact, this effect also occurs among different real-world datasets, i.e. detectors' accuracy drops when the training data (source domain) and the application scenario (target domain) have inherent differences. Therefore, in order to avoid this problem, it is required to adapt the detector trained with synthetic data to operate in the real-world scenario. In this paper, we propose a domain adaptation approach based on boosting LDA exemplar classifiers from both virtual and real worlds. We evaluate our proposal on multiple real-world pedestrian detection datasets. The results show that our method can efficiently adapt the exemplar classifiers from virtual to real world, avoiding drops in average precision over the 15%.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {xvr2013a,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Jiaolong Xu and David Vazquez and Sebastian Ramos and Antonio Lopez and Daniel Ponsa}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Adapting a Pedestrian Detector by Boosting LDA Exemplar Classifiers},<br> &nbsp;&nbsp; booktitle = {CVPR Workshop on Ground Truth - What is a good dataset?}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2013}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/xvr2013a.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2013-01-01"><div class="media"> <a href=".publication-detailXRV2013" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_XRV2013"><img class="img-responsive" src="./Pubs/XRV2013/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2013</span></div><div class="media-body"> <h3><a class="ext_link" href="">DA-DPM PEDESTRIAN DETECTION</a></h3> <h4>ICCV WORKSHOP ON RECONSTRUCTION MEETS RECOGNITION (<b>ICCVW-RR</b>), 2013</h4> <span class="publication_description"></span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_XRV2013"></span> <span class="publication_authors">Jiaolong Xu, Sebastian Ramos, David Vazquez and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailXRV2013 publication-detail"> <div class="image_work" id="image_work_XRV2013"><img class="img-responsive" src="./Pubs/XRV2013/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">DA-DPM PEDESTRIAN DETECTION</h3> <span class="publication_authors">Jiaolong Xu, Sebastian Ramos, David Vazquez and  Antonio M. Lopez</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc"></p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {XRV2013,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Jiaolong Xu and Sebastian Ramos and David Vazquez and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {DA-DPM Pedestrian Detection},<br> &nbsp;&nbsp; booktitle = {ICCV Workshop on Reconstruction meets Recognition}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2013}<br>}</div><a class="ext_link" href=""><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;BOOK CHAPTERS&quot;]" data-date-publication="2012-01-01"><div class="media"> <a href=".publication-detailMGV2012" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_MGV2012"><img class="img-responsive" src="./Pubs/MGV2012/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2012</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/MGV2012a.pdf">PEDESTRIAN DETECTION: EXPLORING VIRTUAL WORLDS</a></h3> <h4>BOOK CHAPTER: HANDBOOK OF PATTERN RECOGNITION: METHODS AND APPLICATION, 2012</h4> <span class="publication_description">Handbook of pattern recognition will include contributions from university educators and active research experts. This Handbook is intended to serve as a basic reference on methods and applications of...</span> </div><hr style="margin:8px auto"> <span class="label label-danger">Book Chapters</span><span class="label selected" id="selected_MGV2012"></span> <span class="publication_authors">Javier Marin, David Geronimo, David Vazquez and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailMGV2012 publication-detail"> <div class="image_work" id="image_work_MGV2012"><img class="img-responsive" src="./Pubs/MGV2012/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">PEDESTRIAN DETECTION: EXPLORING VIRTUAL WORLDS</h3> <span class="publication_authors">Javier Marin, David Geronimo, David Vazquez and  Antonio M. Lopez</span> <span class="label label-danger">Book Chapters</span> <span class="label selected">Selected</span> <p class="project_desc">Handbook of pattern recognition will include contributions from university educators and active research experts. This Handbook is intended to serve as a basic reference on methods and applications of pattern recognition. The primary aim of this handbook is providing the community of pattern recognition with a readable, easy to understand resource that covers introductory, intermediate and advanced topics with equal clarity. Therefore, the Handbook of pattern recognition can serve equally well as reference resource and as classroom textbook. Contributions cover all methods, techniques and applications of pattern recognition. A tentative list of relevant topics might include: 1- Statistical, structural, syntactic pattern recognition. 2- Neural networks, machine learning, data mining. 3- Discrete geometry, algebraic, graph-based techniques for pattern recognition. 4- Face recognition, Signal analysis, image coding and processing, shape and texture analysis. 5- Document processing, text and graphics recognition, digital libraries. 6- Speech recognition, music analysis, multimedia systems. 7- Natural language analysis, information retrieval. 8- Biometrics, biomedical pattern analysis and information systems. 9- Other scientific, engineering, social and economical applications of pattern recognition. 10- Special hardware architectures, software packages for pattern recognition.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INBOOK {MGV2012,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Javier Marin and David Geronimo and David Vazquez and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Pedestrian Detection: Exploring Virtual Worlds},<br> &nbsp;&nbsp; booktitle = {Handbook of Pattern Recognition: Methods and Application}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2012}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/MGV2012a.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2012-01-01"><div class="media"> <a href=".publication-detailSLV2012" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_SLV2012"><img class="img-responsive" src="./Pubs/SLV2012/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2012</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/SLV2012a.pdf">IMPROVING HOG WITH IMAGE SEGMENTATION: APPLICATION TO HUMAN DETECTION</a></h3> <h4>11TH INTERNATIONAL CONFERENCE ON  ADVANCED CONCEPTS FOR INTELLIGENT VISION SYSTEMS (<b>ACIVS</b>), 2012</h4> <span class="publication_description">In this paper we improve the histogram of oriented gradients (HOG), a core descriptor of state-of-the-art object detection, by the use of higher-level information coming from image segmentation. The i...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_SLV2012"></span> <span class="publication_authors">Yainuvis Socarras, David Vazquez, Antonio M. Lopez, David Geronimo and  Theo Gevers</span> </div><div class="mfp-hide mfp-with-anim publication-detailSLV2012 publication-detail"> <div class="image_work" id="image_work_SLV2012"><img class="img-responsive" src="./Pubs/SLV2012/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">IMPROVING HOG WITH IMAGE SEGMENTATION: APPLICATION TO HUMAN DETECTION</h3> <span class="publication_authors">Yainuvis Socarras, David Vazquez, Antonio M. Lopez, David Geronimo and  Theo Gevers</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">In this paper we improve the histogram of oriented gradients (HOG), a core descriptor of state-of-the-art object detection, by the use of higher-level information coming from image segmentation. The idea is to re-weight the descriptor while computing it without increasing its size. The benefits of the proposal are two-fold: (i) to improve the performance of the detector by enriching the descriptor information and (ii) take advantage of the information of image segmentation, which in fact is likely to be used in other stages of the detection system such as candidate generation or refinement.
                  We test our technique in the INRIA person dataset, which was originally developed to test HOG, embedding it in a human detection system. The well-known segmentation method, mean-shift (from smaller to larger super-pixels), and different methods to re-weight the original descriptor (constant, region-luminance, color or texture-dependent) has been evaluated. We achieve performance improvements of 4:47% in detection rate through the use of differences of color between contour pixel neighborhoods as re-weighting function.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {SLV2012,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Yainuvis Socarras and David Vazquez and Antonio Lopez and David Geronimo and Theo Gevers}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Improving HOG with Image Segmentation: Application to Human Detection},<br> &nbsp;&nbsp; booktitle = {11th International Conference on  Advanced Concepts for Intelligent Vision Systems}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2012}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/SLV2012a.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2012-01-01"><div class="media"> <a href=".publication-detailVLP2012" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_VLP2012"><img class="img-responsive" src="./Pubs/VLP2012/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2012</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/VLP2012a.pdf">UNSUPERVISED DOMAIN ADAPTATION OF VIRTUAL AND REAL WORLDS FOR PEDESTRIAN DETECTION</a></h3> <h4>21ST INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (<b>ICPR</b>), 2012</h4> <span class="publication_description">Vision-based object detectors are crucial for different applications. They rely on learnt object models. Ideally, we would like to deploy our vision system in the scenario where it must operate, and l...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_VLP2012"></span> <span class="publication_authors">David Vazquez, Antonio M. Lopez and  Daniel Ponsa</span> </div><div class="mfp-hide mfp-with-anim publication-detailVLP2012 publication-detail"> <div class="image_work" id="image_work_VLP2012"><img class="img-responsive" src="./Pubs/VLP2012/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">UNSUPERVISED DOMAIN ADAPTATION OF VIRTUAL AND REAL WORLDS FOR PEDESTRIAN DETECTION</h3> <span class="publication_authors">David Vazquez, Antonio M. Lopez and  Daniel Ponsa</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">Vision-based object detectors are crucial for different applications. They rely on learnt object models. Ideally, we would like to deploy our vision system in the scenario where it must operate, and lead it to self-learn how to distinguish the  objects of interest, i.e., without human intervention. However, the learning of each object model requires labelled samples collected through a tiresome manual process. For instance, we are interested in exploring the self-training of a pedestrian detector for driver assistance systems. Our first approach to avoid manual labelling consisted in the use of samples coming from realistic computer graphics, so that their labels are automatically available [12]. This would make possible the desired self-training of our pedestrian detector. However, as we showed in [14], between virtual and real worlds it may be a dataset shift. In order to overcome it, we propose the use of unsupervised domain adaptation techniques that avoid human intervention during the adaptation process. In particular, this paper explores the use of the transductive SVM (T-SVM) learning algorithm in order to adapt virtual and real worlds for pedestrian detection (Fig. 1).</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {VLP2012,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {David Vazquez and Antonio Lopez and Daniel Ponsa}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Unsupervised Domain Adaptation of Virtual and Real Worlds for Pedestrian Detection},<br> &nbsp;&nbsp; booktitle = {21st International Conference on Pattern Recognition}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2012}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/VLP2012a.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2011-01-01"><div class="media"> <a href=".publication-detailRVL2011a" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_RVL2011a"><img class="img-responsive" src="./Pubs/RVL2011a/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2011</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/AVL2011b.pdf">OPPONENT COLORS FOR HUMAN DETECTION</a></h3> <h4>5TH IBERIAN CONFERENCE ON PATTERN RECOGNITION AND IMAGE ANALYSIS (<b>IBPRIA</b>), 2011</h4> <span class="publication_description">Human detection is a key component in fields such as advanced driving assistance and video surveillance. However, even detecting non-occluded standing humans remains a challenge of intensive research....</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_RVL2011a"></span> <span class="publication_authors">Muhammad Anwer Rao, David Vazquez and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailRVL2011a publication-detail"> <div class="image_work" id="image_work_RVL2011a"><img class="img-responsive" src="./Pubs/RVL2011a/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">OPPONENT COLORS FOR HUMAN DETECTION</h3> <span class="publication_authors">Muhammad Anwer Rao, David Vazquez and  Antonio M. Lopez</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">Human detection is a key component in fields such as advanced driving assistance and video surveillance. However, even detecting non-occluded standing humans remains a challenge of intensive research.  Finding good features to build human models for further detection is probably one of the most important issues to face. Currently, shape, texture and motion features have deserve extensive attention in the literature. However, color-based features, which are important in other domains (e.g., image categorization), have received much less attention. In fact, the use of RGB color space has become a kind of choice by default. The focus has been put in developing first and second order features on top of RGB space (e.g., HOG and co-occurrence matrices, resp.). In this paper we evaluate the opponent colors (OPP) space as a biologically inspired alternative for human detection. In particular, by feeding OPP space in the baseline framework of Dalal et al. for human detection (based on RGB, HOG and linear SVM), we will obtain better detection performance than by using RGB space. This is a relevant result since, up to the best of our knowledge, OPP space has not been previously used for human detection. This suggests that in the future it could be worth to compute co-occurrence matrices, self-similarity features, etc., also on top of OPP space, i.e., as we have done with HOG in this paper.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {RVL2011a,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Muhammad Anwer Rao and David Vazquez and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Opponent Colors for Human Detection},<br> &nbsp;&nbsp; booktitle = {5th Iberian Conference on Pattern Recognition and Image Analysis}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2011}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/AVL2011b.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2011-01-01"><div class="media"> <a href=".publication-detailRVL2011b" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_RVL2011b"><img class="img-responsive" src="./Pubs/RVL2011b/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2011</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/RVL2011b.pdf">COLOR CONTRIBUTION TO PART-BASED PERSON DETECTION IN DIFFERENT TYPES OF SCENARIOS</a></h3> <h4>14TH INTERNATIONAL CONFERENCE ON COMPUTER ANALYSIS OF IMAGES AND PATTERNS (<b>CAIP</b>), 2011</h4> <span class="publication_description">Camera-based person detection is of paramount interest due to its potential applications. The task is diffcult because the great variety of backgrounds (scenarios, illumination) in which persons are p...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_RVL2011b"></span> <span class="publication_authors">Muhammad Anwer Rao, David Vazquez and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailRVL2011b publication-detail"> <div class="image_work" id="image_work_RVL2011b"><img class="img-responsive" src="./Pubs/RVL2011b/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">COLOR CONTRIBUTION TO PART-BASED PERSON DETECTION IN DIFFERENT TYPES OF SCENARIOS</h3> <span class="publication_authors">Muhammad Anwer Rao, David Vazquez and  Antonio M. Lopez</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">Camera-based person detection is of paramount interest due to its potential applications. The task is diffcult because the great variety of backgrounds (scenarios, illumination) in which persons are present, as well as their intra-class variability (pose, clothe, occlusion). In fact, the class person is one of the included in the popular PASCAL visual object classes (VOC) challenge. A breakthrough for this challenge, regarding person detection, is due to Felzenszwalb et al. These authors proposed a part-based detector that relies on histograms of oriented gradients (HOG) and latent support vector machines (LatSVM) to learn a model of the whole human body and its constitutive parts, as well as their relative position. Since the approach of Felzenszwalb et al. appeared new variants have been proposed, usually giving rise to more complex models. In this paper, we focus on an issue that has not attracted suficient interest up to now. In particular, we refer to the fact that HOG is usually computed from RGB color space, but other possibilities exist and deserve the corresponding investigation. In this paper we challenge RGB space with the opponent color space (OPP), which is inspired in the human vision system.We will compute the HOG on top of OPP, then we train and test the part-based human classifer by Felzenszwalb et al. using PASCAL VOC challenge protocols and person database. Our experiments demonstrate that OPP outperforms RGB. We also investigate possible differences among types of scenarios: indoor, urban and countryside. Interestingly, our experiments suggest that the beneficts of OPP with respect to RGB mainly come for indoor and countryside scenarios, those in which the human visual system was designed by evolution.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {RVL2011b,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Muhammad Anwer Rao and David Vazquez and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Color Contribution to Part-Based Person Detection in Different Types of Scenarios},<br> &nbsp;&nbsp; booktitle = {14th International Conference on Computer Analysis of Images and Patterns}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2011}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/RVL2011b.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2011-01-01"><div class="media"> <a href=".publication-detailVLP2011a" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_VLP2011a"><img class="img-responsive" src="./Pubs/VLP2011a/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2011</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/VLP2011a.pdf">VIRTUAL WORLDS AND ACTIVE LEARNING FOR HUMAN DETECTION</a></h3> <h4>13TH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION (<b>ICMI</b>), 2011</h4> <span class="publication_description">Image based human detection is of paramount interest due to its potential applications in fields such as advanced driving assistance, surveillance and media analysis. However, even detecting non-occlu...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_VLP2011a"></span> <span class="publication_authors">David Vazquez, Antonio M. Lopez, Daniel Ponsa and  Javier Marin</span> </div><div class="mfp-hide mfp-with-anim publication-detailVLP2011a publication-detail"> <div class="image_work" id="image_work_VLP2011a"><img class="img-responsive" src="./Pubs/VLP2011a/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">VIRTUAL WORLDS AND ACTIVE LEARNING FOR HUMAN DETECTION</h3> <span class="publication_authors">David Vazquez, Antonio M. Lopez, Daniel Ponsa and  Javier Marin</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">Image based human detection is of paramount interest due to its potential applications in fields such as advanced driving assistance, surveillance and media analysis. However, even detecting non-occluded standing humans remains a challenge of intensive research. The most promising human detectors rely on classifiers developed in the discriminative paradigm, i.e., trained with labelled samples. However, labeling is a manual intensive step, especially in cases like human detection where it is necessary to provide at least bounding boxes framing the humans for training. To overcome such problem, some authors have proposed the use of a virtual world where the labels of the different objects are obtained automatically. This means that the human models (classifiers) are learnt using the appearance of rendered images, i.e., using realistic computer graphics. Later, these models are used for human detection in images of the real world. The results of this technique are surprisingly good. However, these are not always as good as the classical approach of training and testing with data coming from the same camera, or similar ones. Accordingly, in this paper we address the challenge of using a virtual world for gathering (while playing a videogame) a large amount of automatically labelled samples (virtual humans and background) and then training a classifier that performs equal, in real-world images, than the one obtained by equally training from manually labelled real-world samples. For doing that, we cast the problem as one of domain adaptation. In doing so, we assume that a small amount of manually labelled samples from real-world images is required. To collect these labelled samples we propose a non-standard active learning technique. Therefore, ultimately our human model is learnt by the combination of virtual and real world labelled samples (Fig. 1), which has not been done before. We present quantitative results showing that this approach is valid.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {VLP2011a,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {David Vazquez and Antonio Lopez and Daniel Ponsa and Javier Marin}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Virtual Worlds and Active Learning for Human Detection},<br> &nbsp;&nbsp; booktitle = {13th International Conference on Multimodal Interaction}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2011}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/VLP2011a.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2011-01-01"><div class="media"> <a href=".publication-detailVLP2011b" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_VLP2011b"><img class="img-responsive" src="./Pubs/VLP2011b/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2011</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/VLP2011b.pdf">COOL WORLD: DOMAIN ADAPTATION OF VIRTUAL AND REAL WORLDS FOR HUMAN DETECTION USING ACTIVE LEARNING</a></h3> <h4>NIPS DOMAIN ADAPTATION WORKSHOP: THEORY AND APPLICATION (<b>DA-NIPS</b>), 2011</h4> <span class="publication_description">Image based human detection is of paramount interest for different applications. The most promising human detectors rely on discriminatively learnt classifiers, i.e., trained with labelled samples. Ho...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_VLP2011b"></span> <span class="publication_authors">David Vazquez, Antonio M. Lopez, Daniel Ponsa and  Javier Marin</span> </div><div class="mfp-hide mfp-with-anim publication-detailVLP2011b publication-detail"> <div class="image_work" id="image_work_VLP2011b"><img class="img-responsive" src="./Pubs/VLP2011b/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">COOL WORLD: DOMAIN ADAPTATION OF VIRTUAL AND REAL WORLDS FOR HUMAN DETECTION USING ACTIVE LEARNING</h3> <span class="publication_authors">David Vazquez, Antonio M. Lopez, Daniel Ponsa and  Javier Marin</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">Image based human detection is of paramount interest for different applications. The most promising human detectors rely on discriminatively learnt classifiers, i.e., trained with labelled samples. However, labelling is a manual intensive task, especially in cases like human detection where it is necessary to provide at least bounding boxes framing the humans for training. To overcome such problem, in Marin et al. we have proposed the use of a virtual world where the labels of the different objects are obtained automatically. This means that the human models (classifiers) are learnt using the appearance of realistic computer graphics. Later, these models are used for human detection in images of the real world. The results of this technique are surprisingly good. However, these are not always as good as the classical approach of training and testing with data coming from the same camera and the same type of scenario. Accordingly, in Vazquez et al. we cast the problem as one of supervised domain adaptation. In doing so, we assume that a small amount of manually labelled samples from real-world images is required. To collect these labelled samples we use an active learning technique. Thus, ultimately our human model is learnt by the combination of virtual- and real-world labelled samples which, to the best of our knowledge, was not done before. Here, we term such combined space cool world. In this extended abstract we summarize our proposal, and include quantitative results from Vazquez et al. showing its validity.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {VLP2011b,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {David Vazquez and Antonio Lopez and Daniel Ponsa and Javier Marin}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Cool world: domain adaptation of virtual and real worlds for human detection using active learning},<br> &nbsp;&nbsp; booktitle = {NIPS Domain Adaptation Workshop: Theory and Application}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2011}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/VLP2011b.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;CONFERENCES&quot;]" data-date-publication="2010-01-01"><div class="media"> <a href=".publication-detailMVG2010" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_MVG2010"><img class="img-responsive" src="./Pubs/MVG2010/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2010</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/MVG2010.pdf">LEARNING APPEARANCE IN VIRTUAL SCENARIOS FOR PEDESTRIAN DETECTION</a></h3> <h4>23RD IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (<b>CVPR</b>), 2010</h4> <span class="publication_description">Detecting pedestrians in images is a key functionality to avoid vehicle-to-pedestrian collisions. The most promising detectors rely on appearance-based pedestrian classifiers trained with labelled sam...</span> </div><hr style="margin:8px auto"> <span class="label label-primary">Conferences</span><span class="label selected" id="selected_MVG2010"></span> <span class="publication_authors">Javier Marin, David Vazquez, David Geronimo and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailMVG2010 publication-detail"> <div class="image_work" id="image_work_MVG2010"><img class="img-responsive" src="./Pubs/MVG2010/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">LEARNING APPEARANCE IN VIRTUAL SCENARIOS FOR PEDESTRIAN DETECTION</h3> <span class="publication_authors">Javier Marin, David Vazquez, David Geronimo and  Antonio M. Lopez</span> <span class="label label-primary">Conferences</span> <span class="label selected">Selected</span> <p class="project_desc">Detecting pedestrians in images is a key functionality to avoid vehicle-to-pedestrian collisions. The most promising detectors rely on appearance-based pedestrian classifiers trained with labelled samples. This paper addresses the following question: can a pedestrian appearance model learnt in virtual scenarios work successfully for pedestrian detection in real images? (Fig. 1). Our experiments suggest a positive answer, which is a new and relevant conclusion for research in pedestrian detection. More specifically, we record training sequences in virtual scenarios and then appearance-based pedestrian classifiers are learnt using HOG and linear SVM. We test such classifiers in a publicly available dataset provided by Daimler AG for pedestrian detection benchmarking. This dataset contains real world images acquired from a moving car. The obtained result is compared with the one given by a classifier learnt using samples coming from real images. The comparison reveals that, although virtual samples were not specially selected, both virtual and real based training give rise to classifiers of similar performance.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {MVG2010,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {Javier Marin and David Vazquez and David Geronimo and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Learning Appearance in Virtual Scenarios for Pedestrian Detection},<br> &nbsp;&nbsp; booktitle = {23rd IEEE Conference on Computer Vision and Pattern Recognition}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2010}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/MVG2010.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;THESES&quot;]" data-date-publication="2009-01-01"><div class="media"> <a href=".publication-detailVGL2009" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_VGL2009"><img class="img-responsive" src="./Pubs/VGL2009/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2009</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/VGL2009a.pdf">THE EFFECT OF THE DISTANCE IN PEDESTRIAN DETECTION</a></h3> <h4>CVC TECHNICAL REPORT (<b>M.SC.</b>), 2009</h4> <span class="publication_description">Pedestrian accidents are one of the leading preventable causes of death. In order to reduce the number of accidents, in the last decade the pedestrian protection systems have been introduced, a specia...</span> </div><hr style="margin:8px auto"> <span class="label label-default">Theses</span><span class="label selected" id="selected_VGL2009"></span> <span class="publication_authors">David Vazquez, David Geronimo and  Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailVGL2009 publication-detail"> <div class="image_work" id="image_work_VGL2009"><img class="img-responsive" src="./Pubs/VGL2009/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">THE EFFECT OF THE DISTANCE IN PEDESTRIAN DETECTION</h3> <span class="publication_authors">David Vazquez, David Geronimo and  Antonio M. Lopez</span> <span class="label label-default">Theses</span> <span class="label selected">Selected</span> <p class="project_desc">Pedestrian accidents are one of the leading preventable causes of death. In order to reduce the number of accidents, in the last decade the pedestrian protection systems have been introduced, a special type of advanced driver assistance systems, in witch an on-board camera explores the road ahead for possible collisions with pedestrians in order to warn the driver or perform braking actions. As a result of the variability of the appearance, pose and size, pedestrian detection is a very challenging task. So many techniques, models and features have been proposed to solve the problem. As the appearance of pedestrians varies signicantly as a function of distance, a system based on multiple classiers specialized on diferent depths is likely to improve the overall performance with respect to a typical system based on a general detector. Accordingly, the main aim of this work is to explore the eect of the distance in pedestrian detection. We have evaluated three pedestrian detectors (HOG, HAAR and EOH) in two dierent databases (INRIA and Daimler09) for two dierent sizes (small and big). By a extensive set of experiments we answer to questions like which datasets and evaluation methods are the most adequate, which is the best method for each size of the pedestrians and why or how do the method optimum parameters vary with respect to the distance</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {VGL2009,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {David Vazquez and David Geronimo and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {The effect of the distance in pedestrian detection},<br> &nbsp;&nbsp; booktitle = {CVC Technical Report}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2009}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/VGL2009a.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;THESES&quot;]" data-date-publication="2008-01-01"><div class="media"> <a href=".publication-detailVL2008a" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_VL2008a"><img class="img-responsive" src="./Pubs/VL2008a/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2008</span></div><div class="media-body"> <h3><a class="ext_link" href="">INTRUSION CLASSIFICATION IN INTELLIGENT VIDEO SURVEILLANCE SYSTEMS</a></h3> <h4>ESTUDIS D'ENGINYERIA SUPERIOR EN INFORMÁTICA (<b>PFC</b>), 2008</h4> <span class="publication_description">An intelligent video surveillance system (IVS) is a camera-based installation able to process in real-time the images coming from the cameras. The aim is to automatically warn about different events o...</span> </div><hr style="margin:8px auto"> <span class="label label-default">Theses</span><span class="label selected" id="selected_VL2008a"></span> <span class="publication_authors">David Vazquez, Antonio M. Lopez</span> </div><div class="mfp-hide mfp-with-anim publication-detailVL2008a publication-detail"> <div class="image_work" id="image_work_VL2008a"><img class="img-responsive" src="./Pubs/VL2008a/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">INTRUSION CLASSIFICATION IN INTELLIGENT VIDEO SURVEILLANCE SYSTEMS</h3> <span class="publication_authors">David Vazquez, Antonio M. Lopez</span> <span class="label label-default">Theses</span> <span class="label selected">Selected</span> <p class="project_desc">An intelligent video surveillance system (IVS) is a camera-based installation able to process in real-time the images coming from the cameras. The aim is to automatically warn about different events of interest at the moment they happen. Daview system of Davantis is a com mercial example of IVS system. The problems addressed by any IVS system, and so Daview, are so challenging that none IVS system is perfect, thus, they need continuous improvement. Accordingly, this project aims to study different approaches in order to outperform current Daview performance, in particular, we bet for improving its classification core. We present an in deep study of the state of the art on IVS systems, as well as on how Daview works. Based on that knowledge, we propose four possibilities for improving Daview classification capabilities: improve existent classifiers; improve existing classifiers combination; create new classifiers and create new classifier-based architectures. Our main contribution has been the incorporation of state-of-the-art feature selection and machine learning techniques for the classification tasks, a viewpoint not fully addressed in current Daview system. After a comprehensive quantitative evaluation we will see how one of our proposals clearly outperforms the overall performance of current Daview system. In particular the classification core that we finally propose consists in an AdaBoost One-Against-All architecture that uses appearance and motion features that were already present in current Daview system</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {VL2008a,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {David Vazquez and Antonio Lopez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Intrusion Classification in Intelligent Video Surveillance Systems},<br> &nbsp;&nbsp; booktitle = {Estudis d'Enginyeria Superior en Informática}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2008}<br>}</div><a class="ext_link" href=""><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;THESES&quot;]" data-date-publication="2007-01-01"><div class="media"> <a href=".publication-detailVC2007a" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_VC2007a"><img class="img-responsive" src="./Pubs/VC2007a/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2007</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/VC2007a.pdf">EMPLEO DE SISTEMAS BIOMÉTRICOS FACIALES APLICADOS AL RECONOCIMIENTO DE PERSONAS EN AEROPUERTOS</a></h3> <h4>INGENIERÍA TÉCNICA EN INFORMÁTICA DE SISTEMAS (<b></b>), 2007</h4> <span class="publication_description">El presente proyecto se desarrolló a lo largo del año 2005 y 2006, probando un prototipo de un sistema de verificación facial con imágenes extraídas de las cámaras de video-vigilancia del aeropuerto d...</span> </div><hr style="margin:8px auto"> <span class="label label-default">Theses</span><span class="label selected" id="selected_VC2007a"></span> <span class="publication_authors">David Vazquez, Enrique Cabello</span> </div><div class="mfp-hide mfp-with-anim publication-detailVC2007a publication-detail"> <div class="image_work" id="image_work_VC2007a"><img class="img-responsive" src="./Pubs/VC2007a/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">EMPLEO DE SISTEMAS BIOMÉTRICOS FACIALES APLICADOS AL RECONOCIMIENTO DE PERSONAS EN AEROPUERTOS</h3> <span class="publication_authors">David Vazquez, Enrique Cabello</span> <span class="label label-default">Theses</span> <span class="label selected">Selected</span> <p class="project_desc">El presente proyecto se desarrolló a lo largo del año 2005 y 2006, probando un prototipo de un sistema de verificación facial con imágenes extraídas de las cámaras de video-vigilancia del aeropuerto de Barajas. Se diseñaron varios experimentos, agrupados en dos clases. En el primer tipo, el sistema es entre- nado con imágenes obtenidas en condiciones de laboratorio y luego probado con imágenes extraídas de las cámaras de video-vigilancia del aeropuerto de Barajas. En el segundo caso, tanto las imágenes de entrenamiento como las de prueba corresponden a imágenes extraídas de Barajas.
                  Se ha desarrollado un sistema completo, que incluye adquisición y digitalización de las imágenes, localización y recorte de las caras en escena, verificación de sujetos y obtención de resultados. Los resultados muestran que, en general, un sistema de verificación facial basado en imágenes puede ser una valiosa ayuda a un operario que deba estar vigilando amplias zonas.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@INPROCEEDINGS {VC2007a,<br> &nbsp;&nbsp; author &nbsp;&nbsp;&nbsp; = {David Vazquez and Enrique Cabello}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Empleo de sistemas biométricos faciales aplicados al reconocimiento de personas en aeropuertos},<br> &nbsp;&nbsp; booktitle = {Ingeniería Técnica en Informática de Sistemas}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {2007}<br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/VC2007a.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div><div class="publication_item" data-groups="[&quot;all&quot;,&quot;JOURNAL PAPERS&quot;]" data-date-publication="2006-01-01"><div class="media"> <a href=".publication-detailCCS2006a" class="ex-link open_popup" data-effect="mfp-zoom-out"><i class="fa fa-plus-square-o"></i></a><div class="date pull-left"> <span class="day" id="mini_image_CCS2006a"><img class="img-responsive" src="./Pubs/CCS2006a/teaser.jpg" alt="img" height="200" width="200"></span> <span class="year">2006</span></div><div class="media-body"> <h3><a class="ext_link" href="http://refbase.cvc.uab.es/files/CCS2006a.pdf">EMPLEO DE SISTEMAS BIOMÉTRICOS PARA EL RECONOCIMIENTO DE PERSONAS EN AEROPUERTOS</a></h3> <h4>INSTITUTO UNIVERSITARIO DE INVESTIGACIÓN SOBRE SEGURIDAD INTERIOR (IUSI 2006) (<b></b>), 2006</h4> <span class="publication_description">El presente proyecto se desarrolló a lo largo del año 2005, probando un prototipo de un sistema de verificación facial con imágenes extraídas de las cámaras de video vigilancia del aeropuerto de Baraj...</span> </div><hr style="margin:8px auto"> <span class="label label-success">Journal papers</span><span class="label selected" id="selected_CCS2006a">Selected</span> <span class="publication_authors">Enrique Cabello, Cristina Conde, Angel Serrano, Licesio Rodriguez and  David Vazquez</span> </div><div class="mfp-hide mfp-with-anim publication-detailCCS2006a publication-detail"> <div class="image_work" id="image_work_CCS2006a"><img class="img-responsive" src="./Pubs/CCS2006a/teaser.jpg" alt="img" height="200" width="480"></div><div class="project_content"> <h3 class="publication_title">EMPLEO DE SISTEMAS BIOMÉTRICOS PARA EL RECONOCIMIENTO DE PERSONAS EN AEROPUERTOS</h3> <span class="publication_authors">Enrique Cabello, Cristina Conde, Angel Serrano, Licesio Rodriguez and  David Vazquez</span> <span class="label label-success">Journal papers</span> <span class="label selected">Selected</span> <p class="project_desc">El presente proyecto se desarrolló a lo largo del año 2005, probando un prototipo de un sistema de verificación facial con imágenes extraídas de las cámaras de video vigilancia del aeropuerto de Barajas. Se diseñaron varios experimentos, agrupados en dos clases. En el primer tipo, el sistema es entrenado con imágenes obtenidas en condiciones de laboratorio y luego probado con imágenes extraídas de las cámaras de video vigilancia del aeropuerto de Barajas. En el segundo caso, tanto las imágenes de entrenamiento como las de prueba corresponden a imágenes extraídas de Barajas. Se ha desarrollado un sistema completo, que incluye adquisición y digitalización de las imágenes, localización y recorte de las caras en escena, verificación de sujetos y obtención de resultados. Los resultados muestran, que, en general, un sistema de verificación facial basado en imágenes puede ser una ayuda a un operario que deba estar vigilando amplias zonas.</p> </div><div class="bibtex"> <span class="bibtitle"> <b>Latex Bibtex Citation:</b></span><br>@ARTICLE {CCS2006a,<br> &nbsp;&nbsp; author &nbsp;= {Enrique Cabello and Cristina Conde and Angel Serrano and Licesio Rodriguez and David Vazquez}, <br> &nbsp;&nbsp; title &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = {Empleo de sistemas biométricos para el reconocimiento de personas en aeropuertos},<br> &nbsp;&nbsp; journal = {Instituto Universitario de Investigación sobre Seguridad Interior (IUSI 2006)}, <br> &nbsp;&nbsp; year &nbsp;&nbsp;&nbsp;&nbsp; = {2006}, <br> &nbsp;&nbsp; volume = {}, <br> &nbsp;&nbsp; issue &nbsp;&nbsp;&nbsp; = {}, <br> &nbsp;&nbsp; pages &nbsp;&nbsp; = {}  <br>}</div><a class="ext_link" href="http://refbase.cvc.uab.es/files/CCS2006a.pdf"><i class="fa fa-external-link"></i></a> <div style="clear:both"></div></div> </div>

              </div>

              <!-- <div class="hide-scroll">
                <div class="viewport" id="mygrid2">
                </div>
            </div> -->
          <!--  <script type="text/javascript"> mainF('author=David%20Vazquez'); </script> -->
              <!-- End Publication Wrapper -->
	        </div>
            <div class="clear"></div>
          </article>
          <!-- End Publication Section -->

          <!-- Research Section -->
          <article class="hs-content research-section" id="section4"> <span class="sec-icon fa fa-flask"></span>
            <div class="hs-inner"> <span class="before-title">.04</span>
              <h2>RESEARCH</h2>
              <!--
              <span class="content-title">LABORATORY TEAM</span>
              <div class="team_wrapper">
                <div class="team-card-container">
                  <div class="card">
                    <div class="front team1">
                      <div class="front-detail">
                        <h4>JOHN DOE</h4>
                        <h3>RESEARCH ASSISTANT</h3>
                      </div>
                    </div>
                    <div class="back">
                      <p>Lorem ipsum dolor sit amet, consectetur adipiscingVivam
                        sit amet ligula non lectus cursus egestas. Cras erat
                        lorem, fringilla quis sagittis in, sagittis inNam leo
                        tortor Nam leo tortor Vivam.</p>
                      <div class="social-icons"> <a href="#"><i class="fa fa-facebook"></i></a>
                        <a href="#"><i class="fa fa-twitter"></i></a> <a href="#"><i
                            class="fa fa-linkedin"></i></a> <a href="#"><i class="fa fa fa-dribbble"></i></a>
                        <a href="#"><i class="fa fa fa-github"></i></a> </div>
                    </div>
                  </div>
                </div>
                <div class="team-card-container">
                  <div class="card">
                    <div class="front team2">
                      <div class="front-detail">
                        <h4>JENNIFER DOE</h4>
                        <h3>ASSOCIATE PROFESSOR</h3>
                      </div>
                    </div>
                    <div class="back">
                      <p>Lorem ipsum dolor sit amet, consectetur adipiscingVivam
                        sit amet ligula non lectus cursus egestas. Cras erat
                        lorem, fringilla quis sagittis in, sagittis inNam leo
                        tortor Nam leo tortor Vivam.</p>
                      <div class="social-icons"> <a href="#"><i class="fa fa-facebook"></i></a>
                        <a href="#"><i class="fa fa-twitter"></i></a> <a href="#"><i
                            class="fa fa-linkedin"></i></a> <a href="#"><i class="fa fa fa-dribbble"></i></a>
                        <a href="#"><i class="fa fa fa-github"></i></a> </div>
                    </div>
                  </div>
                </div>
                <div class="team-card-container">
                  <div class="card">
                    <div class="front team3">
                      <div class="front-detail">
                        <h4>JOHNATAN DOE</h4>
                        <h3>SENIOR RESEARCH TECHNICIAN</h3>
                      </div>
                    </div>
                    <div class="back">
                      <p>Lorem ipsum dolor sit amet, consectetur adipiscingVivam
                        sit amet ligula non lectus cursus egestas. Cras erat
                        lorem, fringilla quis sagittis in, sagittis inNam leo
                        tortor Nam leo tortor Vivam.</p>
                      <div class="social-icons"> <a href="#"><i class="fa fa-facebook"></i></a>
                        <a href="#"><i class="fa fa-twitter"></i></a> <a href="#"><i
                            class="fa fa-linkedin"></i></a> <a href="#"><i class="fa fa fa-dribbble"></i></a>
                        <a href="#"><i class="fa fa fa-github"></i></a> </div>
                    </div>
                  </div>
                </div>
                <div class="team-card-container">
                  <div class="card">
                    <div class="front team4">
                      <div class="front-detail">
                        <h4>CATHERINE DOE</h4>
                        <h3>RESEARCH FELLOW</h3>
                      </div>
                    </div>

                    <div class="back">
                      <p>Lorem ipsum dolor sit amet, consectetur adipiscingVivam
                        sit amet ligula non lectus cursus egestas. Cras erat
                        lorem, fringilla quis sagittis in, sagittis inNam leo
                        tortor Nam leo tortor Vivam.</p>
                      <div class="social-icons"> <a href="#"><i class="fa fa-facebook"></i></a>
                        <a href="#"><i class="fa fa-twitter"></i></a> <a href="#"><i
                            class="fa fa-linkedin"></i></a> <a href="#"><i class="fa fa fa-dribbble"></i></a>
                        <a href="#"><i class="fa fa fa-github"></i></a> </div>
                    </div>
                  </div>
                </div>
              </div>
          -->
              <span class="content-title">PUBLIC RESEARCH PROJECTS</span>
              <div class="slide-wrapper">
                <nav> <a class="fa fa-angle-left" id="btn-prev"></a> <a class="fa fa-angle-right"
                    id="btn-next"></a> </nav>
                <div class="slider-details"> <span class="slide-counter"></span>
                  <span class="slide-date"><i class="li_calendar"></i></span> </div>
                <div class="slide active" data-date="2015/2017">
                  <div class="slide-header">
                    <a href="http://adas.cvc.uab.es/elektra" target="_blank"><h3>ELEKTRA</h3></a>
                    <h4>AUTONOMOUS VEHICLE</h4>
                  </div>
                  <div class="slide-content">
                    <img class="img-responsive" src="images/projects/elektra.png" alt="img" height="200" width="480">
                    <p><a href="http://adas.cvc.uab.es/elektra" target="_blank">Elektra</a> is an autonomous driving platform formed by more than 20 professionals from different backgrounds: CVC-UAB (Environment perception), CAOS-UAB (Enmbedded hardware) UPC-Tarrasa (Control & planning), CTTC-UPC (Positioning), UAB-DEIC (Communications), UAB-CEPHIS (Electronics), CT Ingenieros (Vehicle enginering) and the municipality of Sant Quirze - Barcelona (vehicle testing). Elektra pretends to be the Catalan hub of autonomous driving, with a pool of professionals with whom to carry out research applied to intelligent mobility and technology transfer.</p>
                    <p>The project is highly relying on computer vision techniques for perception (stereo, stixels, obstacle detection, scene understanding) which tend to be computationally demanding, localization (GPS + IMU and vision) and navigation (Control and Planning).</p>
                    <p>In order to have a car that can drive, several things are needed. Firstly, accurate pedestrian (obstacle) detection. Secondly, free navigable space detection, which is no more than detecting the lane without obstacles or interferences. Thirdly, localization. The car needs to know where it is at and where it is going towards. Fourthly, planning. The car has to plan its way from point A to point B in the smoothest way possible and thus define a global trajectory. And last but not least, control: to execute the motion plan performing the necessary manoeuvres.</p>
                  </div>
                </div>
                <div class="slide" data-date="2016/2017">
                  <div class="slide-header">
                    <a href="http://www.synthia-dataset.net" target="_blank"><h3>SYNTHIA</h3></a>
                    <h4>AUTONOMOUS DRIVING SIMULATOR</h4>
                  </div>
                  <div class="slide-content">
                    <img class="img-responsive" src="images/projects/synthia.png" alt="img" height="200" width="480">
                    <p><a href="http://www.synthia-dataset.net" target="_blank">SYNTHIA</a> has been one of our latest Projects, where we have created a driving simulator in order to teach driverless cars how to drive. The simulator has been licensed to various international companies.</p>
                    <p>SYNTHIA allows to generate datasets with the purpose of aiding scene understanding problems in the context of driving scenarios. The datasets consists of photo-realistic images rendered from a virtual city and comes with precise pixel-level semantic class and instance annotations (currently, it is cityscapes compatible), depth and optical flow. The CVC team is constantly incrementing the functionalities of SYNTHIA for generating more types of ground truth. Different conditions can be forced such as illumination, weather, season, and camera locations (multi-camera calibrated settings are possible). The current version of SYNTHIA allows generating more than 10000 images per hour with such a ground truth, using a game-graded regular GPU.</p>
                  </div>
                </div>
                <div class="slide" data-date="2015/2017">
                  <div class="slide-header">
                    <a href="http://adas.cvc.uab.es/projects/acdc/" target="_blank"><h3>ACDC</h3></a>
                    <h4>AUTONOMOUS COOPERATIVE DRIVING IN THE CITY</h4>
                  </div>
                  <div class="slide-content">
                    <img class="img-responsive" src="images/projects/acdc.png" alt="img" height="200" width="480">
                    <p>The massive use of automobiles has been a major benefit in terms of personal mobility and sense of freedom, but at the expense of significant drawbacks: traffic accidents and environmental pollution; both factors ending up in health issues and in a huge economic cost. These considerations lead us to think that personal automobiles may not fit in the future cities that industrialized countries have been designing for years.</p>
                    <p>Accordingly, in addition to improving the current public system focused on transporting masses at once (trams, metro), we imagined a centralized system in the city receiving mobility petitions from users all around. The city would control a fleet of automated (driverless) vehicles that would cooperate among them and with other elements such as infrastructure surveillance cameras, traffic lights, etc., to move safely, comfortably, and therefore saving energy and minimizing congestion. The “Automated and Cooperative Driving in the City (ACDC)” proposal was our first step towards this dream. The project started in 2015 after receiving funding from the Spanish Ministry of Economy and is planned to end in 2018.</p>
                    <p>ACDC has been focused on developing advanced software for data processing coming from relatively cheap sensors, rather than assuming the use of expensive sensors just to reduce the complexity of interpreting the data. Thus, from the scientific point of view, ACDC is in the realm of artificial intelligence, machine learning, planning and control, as well as computer vision and general sensor information processing and fusion.</p>
                  </div>
                </div>
                <div class="slide" data-date="2012/2015">
                  <div class="slide-header">
                    <a href="http://adas.cvc.uab.es/projects/eco-drivers/" target="_blank"><h3>ECO-DRIVERS</h3></a>
                    <h4>COOPERATIVE DRIVING</h4>
                  </div>
                  <div class="slide-content">
                    <img class="img-responsive" src="images/projects/ecodriver.png" alt="img" height="200" width="480">
                    <p>The aim of <a href="http://adas.cvc.uab.es/projects/eco-drivers/" target="_blank">eCo-Drivers</a> (Ecologic Cooperative Driver and Road Intelligent Visual Exploration for Route Safety) was to deepen into the research of technologies for bringing Automatic Driving Assistance Systems to urban oriented electric vehicles. The features of this proposal were mainly two: the use of vision as an “eco-sensor” and, secondly, a driver-centric approach. Rather than thinking in road and driver monitoring as working-alone ADAS, we made both technologies cooperate and so assist the driver only when necessary, working as actual co-drivers.</p>
                    <p>In order to accomplish our goals within this project, we decided to acquire our electric car and thus obtain a prototype in which to test our research. This project was, in fact, the fusion of three complementary and collaborative subprojects (1) “Vision-based Driver Assistance Systems for Urban Environments (ViDAS-UrbE)”; (2) “Driver Distraction Detection System (D3System)”; and (3) “Intelligent Agent-based Driver Decision Support (i-Support)”. The core research of subprojects (1) and (2) focused on Computer Vision, while (3) addressed research on machine learning and reasoning under uncertain and incomplete data. Based on this research, the project aimed to develop two urban-oriented and vision-based co-drivers for both a driver-centric obstacle detection; and a driver-centric pedestrian detection.</p>
                  </div>
                </div>
                <div class="slide" data-date="2013/2014">
                  <div class="slide-header">
                    <a href="http://mapea2.cvc.uab.es" target="_blank"><h3>MAPEA2</h3></a>
                    <h4>MAPPING PEDESTRIAN BEHAVIOUR</h4>
                  </div>
                  <div class="slide-content">
                    <img class="img-responsive" src="images/projects/mapea2.png" alt="img" height="200" width="480">
                    <p>Reducing vehicle accidents with pedestrians is one of the aims within vehicle security. In fact, it is know that, in order to obtain a 5 star EuroNCAP, it will be necessary to incorporate pedestrian detectors (PDs) within vehicles, with the aim to avoid the highest number of accidents possible. In this Project, ‘Mapping pedestrian behaviour’ we proposed to adapt our Pedestrian detector to create risk maps in relation to accidents.</p>
                    <p>These maps will be crucial information for more advanced Pedestrian detectors, for future autonomous cars which work in cities having door to door trajectories. The system we wanted to develop pretended to be compact and easily installable in any car in a non-invasive way and without the need of complex calibration processes. This restriction is crucial in order to create risk maps with a higher rapidness and lower cost. As a proof of concept, within MAPEA2, we have planned to create the pedestrian accident risk map within the Bellaterra Campus of the Autonomous University of Barcelona (in which the CVC is located).</p>
                  </div>
                </div>
              </div>
            </div>
          </article>
          <!-- End Research Section -->


          <!-- Teaching Section -->
          <article class="hs-content teaching-section" id="section5"> <span class="sec-icon fa fa-book"></span>
            <div class="hs-inner"> <span class="before-title">.05</span>
              <h2>TEACHING</h2>
              <div class="teaching-wrapper">
                <ul class="teaching">
                  <li class="time-label"> <span class="content-title">CURRENT</span>
                  </li>
                  <li>
                    <div class="teaching-tag"> <span class="fa fa-suitcase"></span>
                      <div class="teaching-date"> <span>NOW</span>
                        <div class="separator"></div>
                        <span>2008</span> </div>
                    </div>
                    <div class="timeline-item">
                      <h3 class="timeline-header">ASSISTANT PROFESSOR</h3>
                      <div class="timeline-body">
                        <h4>AUTONOMOUS UNIVERSITY OF BARCELONA</h4>
                        <span>Master of Computer Vision. Coordinator of the Deep Learning modules.</span> </div>
                    </div>
                  </li>
                  <li class="time-label"> <span class="content-title">TEACHING
                      HISTORY</span> </li>
                  <li>
                    <div class="teaching-tag"> <span class="fa fa-suitcase"></span>
                      <div class="teaching-date"> <span>2009</span>
                        <div class="separator"></div>
                        <span>2010</span> </div>
                    </div>
                    <div class="timeline-item">
                      <h3 class="timeline-header">ASSISTANT PROFESSOR</h3>
                      <div class="timeline-body">
                        <h4>AUTONOMOUS UNIVERSITY OF BARCELONA</h4>
                        <span>Teaching in the subjects: Artificial Intelligence (I & II), Master of Computer Vision, Software Degign & Software Engineering.</span> </div>
                    </div>
                  </li>
                </ul>
              </div>
            </div>
          </article>
          <!-- End Teaching Section -->

          <!-- Skills Section -->
          <article class="hs-content skills-section" id="section6"> <span class="sec-icon fa fa-diamond"></span>
            <div class="hs-inner"> <span class="before-title">.06</span>
              <h2>SKILLS</h2>
              <span class="content-title">PROGRAMMING SKIILLS</span>
              <div class="skolls">
                  <span class="skill-description">I started programming as a child. I'm passionate about software engineering. My favourite languajes are C++, Python, TensorFlow and Keras.</span>
                <div class="bar-main-container">
                  <div class="wrap">
                    <div class="bar-percentage" data-percentage="90"></div>
                    <span class="skill-detail"><i class="fa fa-bar-chart"></i>LEVEL: EXPERT</span>
                    <span class="skill-detail"><i class="fa fa-binoculars"></i>EXPERIENCE: 10 YEARS</span>
                    <div class="bar-container"><div class="bar"></div></div>
                    <span class="label">C/C++</span>
                    <span class="label">OpenCV</span>
                    <span class="label">Matlab</span>
                    <div style="clear:both;"></div>
                  </div>
                  <br>
                  <div class="wrap">
                    <div class="bar-percentage" data-percentage="80"></div>
                    <span class="skill-detail"><i class="fa fa-bar-chart"></i>LEVEL: ADVANCED</span>
                    <span class="skill-detail"><i class="fa fa-binoculars"></i>EXPERIENCE: 3 YEARS</span>
                    <div class="bar-container"><div class="bar"></div></div>
                    <span class="label">Python</span>
                    <span class="label">Numpy</span>
                    <span class="label">Java</span>
                    <span class="label">CUDA</span>
                    <span class="label">GitHub</span>
                    <div style="clear:both;"></div>
                  </div>
                  <br>
                  <div class="wrap">
                    <div class="bar-percentage" data-percentage="75"></div>
                    <span class="skill-detail"><i class="fa fa-bar-chart"></i>LEVEL: INTERMEDIATE</span>
                    <span class="skill-detail"><i class="fa fa-binoculars"></i>EXPERIENCE: 2 YEARS</span>
                    <div class="bar-container"><div class="bar"></div></div>
                    <span class="label">Theano</span>
                    <span class="label">Tensorflow</span>
                    <span class="label">Lasagne</span>
                    <span class="label">Keras</span>
                    <span class="label">Caffe</span>
                    <div style="clear:both;"></div>
                  </div>
                </div>
              </div>
              <span class="content-title">COMPUTER VISION AND MACHINE LEARNING SKILLS</span>
              <div class="skolls">
                  <span class="skill-description">I'm profficient solving computer vision problems by using machine learning techniques.</span>
                <div class="bar-main-container">
                  <div class="wrap">
                    <div class="bar-percentage" data-percentage="95"></div>
                    <span class="skill-detail"><i class="fa fa-bar-chart"></i>LEVEL: EXPERT</span>
                    <span class="skill-detail"><i class="fa fa-binoculars"></i>EXPERIENCE: 10 YEARS</span>
                    <div class="bar-container"><div class="bar"></div></div>
                    <span class="label">3D Reconstruction</span>
                    <span class="label">Recognition</span>
                    <span class="label">Detection</span>
                    <span class="label">Semantic Segmentation</span>
                    <span class="label">Image Generation</span>
                    <div style="clear:both;"></div>
                  </div>
                  <br>
                  <br>
                  <div class="wrap">
                    <div class="bar-percentage" data-percentage="80"></div>
                    <span class="skill-detail"><i class="fa fa-bar-chart"></i>LEVEL: ADVANCED</span>
                    <span class="skill-detail"><i class="fa fa-binoculars"></i>EXPERIENCE: 10 YEARS</span>
                    <div class="bar-container"><div class="bar"></div></div>
                    <span class="label">Deep Learning</span>
                    <span class="label">SVM</span>
                    <span class="label">RF</span>
                    <span class="label">AdaBoost</span>
                    <div style="clear:both;"></div>
                  </div>
                </div>
              </div>

              <span class="content-title">DESIGN AND OFFICE SKILLS</span>
              <div class="skolls">
                  <span class="skill-description">I enjoy designing websites and using collaborative tools such GitHub or Overleaf.</span>
                <div class="bar-main-container">
                  <div class="wrap">
                    <div class="bar-percentage" data-percentage="80"></div>
                    <span class="skill-detail"><i class="fa fa-bar-chart"></i>LEVEL: INTERMEDIATE</span>
                    <span class="skill-detail"><i class="fa fa-binoculars"></i>EXPERIENCE: 5 YEARS</span>
                    <div class="bar-container"><div class="bar"></div></div>
                    <span class="label">Html</span>
                    <span class="label">Php</span>
                    <span class="label">Css</span>
                    <span class="label">Wordpress</span>
                    <div style="clear:both;"></div>
                  </div>
                  <br>
                  <div class="wrap">
                    <div class="bar-percentage" data-percentage="95"></div>
                    <span class="skill-detail"><i class="fa fa-bar-chart"></i>LEVEL: ADVANCED</span>
                    <span class="skill-detail"><i class="fa fa-binoculars"></i>EXPERIENCE: 10 YEARS</span>
                    <div class="bar-container"><div class="bar"></div></div>
                    <span class="label">Office</span>
                    <span class="label">LaTex</span>
                    <span class="label">Overleaf</span>
                    <div style="clear:both;"></div>
                  </div>
                </div>
              </div>

              <span class="content-title">SOCIAL SKILLS</span>
              <div class="skolls">
                  <span class="skill-description">I'm focused on research & development. I have a strong sense of leadership (I'm leading the autonomous driving vehicle Elektra); Highly organized; dynamic; excellent planning skills with great attention to detail and ability to prioritize work.</span>
                <div class="bar-main-container">
                  <div class="wrap">
                    <div class="bar-percentage" data-percentage="80"></div>
                    <span class="skill-detail"><i class="fa fa-bar-chart"></i>LEVEL: INTERMEDIATE</span>
                    <span class="skill-detail"><i class="fa fa-binoculars"></i>EXPERIENCE: 5 YEARS</span>
                    <div class="bar-container"><div class="bar"></div></div>
                    <span class="label">Research</span>
                    <span class="label">Analytical thinking</span>
                    <span class="label">Open-minded</span>
                    <span class="label">Adaptability</span>
                    <span class="label">Leadership</span>
                    <span class="label">Communication</span>
                    <!-- http://talentegg.ca/incubator/2013/12/25/6-skills-employers-resume/ -->
                    <div style="clear:both;"></div>
                  </div>
                  <br>
                </div>
              </div>
            </div>
          </article>
          <!-- End Skills Section -->
          <!-- Works Section -->
          <article class="hs-content works-section" id="section7"> <span class="sec-icon fa fa-archive"></span>
            <div class="hs-inner"> <span class="before-title">.07</span>
              <h2>WORKS</h2>
              <div class="portfolio">
                <!-- Portfolio Item -->
                <figure class="effect-milo">
                    <img src="images/works/elektra.png" alt="img11" height="222" width="282">
                    <figcaption> <span class="label">Autonomous Driving</span>
                    <div class="portfolio_button">
                      <h3>Elektra Autonomous Vehicle</h3>
                      <a href=".work1" class="open_popup" data-effect="mfp-zoom-out"><i class="hovicon effect-9 sub-b"><i class="fa fa-search"></i></i></a>
                    </div>
                    <div class="mfp-hide mfp-with-anim work_desc work1">
                      <div class="col-md-6">
                        <div class="image_work">
                            <!-- <iframe width="100%" height=0 src="https://www.youtube.com/embed/K0omhJXawTo" frameborder="0" allowfullscreen></iframe> -->
                            <div class="video_container"> <iframe src="https://www.youtube.com/embed/K0omhJXawTo" frameborder="0" allowfullscreen class="video_iframe"></iframe> </div>
                        </div>
                      </div>
                      <div class="col-md-6">
                        <div class="project_content">
                          <h2 class="project_title">Elektra Autonomous Vehicle</h2>
                          <p class="project_desc">Elektra is an autonomous driving platform formed by more than 20 professionals from different backgrounds: CVC-UAB (Environment perception), CAOS-UAB (Enmbedded hardware) UPC-Tarrasa (Control & planning), CTTC-UPC (Positioning), UAB-DEIC (Communications), UAB-CEPHIS (Electronics), CT Ingenieros (Vehicle enginering) and the municipality of Sant Quirze - Barcelona (vehicle testing). Elektra pretends to be the Catalan hub of autonomous driving, with a pool of professionals with whom to carry out research applied to intelligent mobility and technology transfer. <br><br>
                          The project is highly relying on computer vision techniques for perception (stereo, stixels, obstacle detection, scene understanding) which tend to be computationally demanding, localization (GPS + IMU and vision) and navigation (Control and Planning). <br><br>
                          In order to have a car that can drive, several things are needed. Firstly, accurate pedestrian (obstacle) detection. Secondly, free navigable space detection, which is no more than detecting the lane without obstacles or interferences. Thirdly, localization. The car needs to know where it is at and where it is going towards. Fourthly, planning. The car has to plan its way from point A to point B in the smoothest way possible and thus define a global trajectory. And last but not least, control: to execute the motion plan performing the necessary manoeuvres.</p>
                        </div>
                      </div>
                      <a class="ext_link" href="http://adas.cvc.uab.es/elektra"><i class="fa fa-external-link"></i></a>
                      <div style="clear:both"></div>
                    </div>
                  </figcaption> </figure>
                <!-- End Portfolio Item -->

                <!-- Portfolio Item -->
                <figure class="effect-milo">
                    <img src="images/works/sgm.png" alt="img11" height="222" width="282">
                    <figcaption> <span class="label">3D recostruction</span>
                    <div class="portfolio_button">
                      <h3>Stereo Disparity</h3>
                      <a href=".work2" class="open_popup" data-effect="mfp-zoom-out"><i class="hovicon effect-9 sub-b"><i class="fa fa-search"></i></i></a>
                    </div>
                    <div class="mfp-hide mfp-with-anim work_desc work2">
                      <div class="col-md-6">
                        <div class="image_work">
                            <div class="video_container"> <iframe src="https://www.youtube.com/embed/sKqQni5LhYo" frameborder="0" allowfullscreen class="video_iframe"></iframe> </div>
                        </div>
                      </div>
                      <div class="col-md-6">
                        <div class="project_content">
                          <h2 class="project_title">Semi-Global Matching</h2>
                          <p class="project_desc">Dense, robust and real-time computation of depth information from stereo-camera systems is a computationally demanding requirement for robotics, advanced driver assistance systems (ADAS) and autonomous vehicles. Semi-Global Matching (SGM) is a widely used algorithm that propagates consistency constraints along several paths across the image. This work presents a real-time system producing reliable disparity estimation results on the new embedded energyefficient GPU devices. Our design runs on a Tegra X1 at 42 frames per second (fps) for an image size of 640×480, 128 disparity levels, and using 4 path directions for the SGM method.</p>
                        </div>
                      </div>
                      <a class="ext_link" href="https://github.com/dhernandez0/sgm"><i class="fa fa-external-link"></i></a>
                      <div style="clear:both"></div>
                    </div>
                  </figcaption> </figure>
                <!-- End Portfolio Item -->

                <!-- Portfolio Item -->
                <figure class="effect-milo">
                    <img src="images/works/stixels.png" alt="img11" height="222" width="282">
                    <figcaption> <span class="label">Stixel Representation</span>
                    <div class="portfolio_button">
                      <h3>Stixel Representation in the GPU</h3>
                      <a href=".work3" class="open_popup" data-effect="mfp-zoom-out"><i class="hovicon effect-9 sub-b"><i class="fa fa-search"></i></i></a>
                    </div>
                    <div class="mfp-hide mfp-with-anim work_desc work3">
                      <div class="col-md-6">
                        <div class="image_work">
                            <div class="video_container"> <iframe src="https://www.youtube.com/embed/RgAkfbA0sSY" frameborder="0" allowfullscreen class="video_iframe"></iframe> </div>
                        </div>
                      </div>
                      <div class="col-md-6">
                        <div class="project_content">
                          <h2 class="project_title">GPU-accelerated real-time stixel computation</h2>
                          <p class="project_desc">The Stixel World is a medium-level, compact representation of road scenes that abstracts millions of disparity pixels into hundreds or thousands of stixels. The goal of this work is to implement and evaluate a complete multi-stixel estimation pipeline on an embedded, energy-efficient, GPU-accelerated device. This work presents a full GPU-accelerated implementation of stixel estimation that produces reliable results at 26 frames per second (real-time) on the Tegra X1 for disparity images of 1024x440 pixels and stixel widths of 5 pixels, and achieves more than 400 frames per second on a high-end Titan X GPU card.</p>
                        </div>
                      </div>
                      <a class="ext_link" href="https://github.com/dhernandez0/stixels"><i class="fa fa-external-link"></i></a>
                      <div style="clear:both"></div>
                    </div>
                  </figcaption> </figure>
                <!-- End Portfolio Item -->

                <!-- Portfolio Item -->
                <figure class="effect-milo">
                    <img src="images/works/polyps.jpg" alt="img11" height="222" width="282">
                    <figcaption> <span class="label">Polyp Segmentation</span>
                    <div class="portfolio_button">
                      <h3>Image Semantic Segmentation</h3>
                      <a href=".work4" class="open_popup" data-effect="mfp-zoom-out"><i class="hovicon effect-9 sub-b"><i class="fa fa-search"></i></i></a>
                    </div>
                    <div class="mfp-hide mfp-with-anim work_desc work4">
                      <div class="col-md-6">
                        <div class="image_work">
                            <img src="images/works/polyps.jpg" alt="img" height="420" width="560">
                            <!-- <div class="video_container"> <iframe src="https://www.youtube.com/embed/RgAkfbA0sSY" frameborder="0" allowfullscreen class="video_iframe"></iframe> </div> -->
                        </div>
                      </div>
                      <div class="col-md-6">
                        <div class="project_content">
                          <h2 class="project_title">A Benchmark for Endoluminal Scene Segmentation of Colonoscopy Images</h2>
                          <p class="project_desc">Colorectal cancer (CRC) is the third cause of cancer death worldwide. Currently, the standard approach to reduce CRC-related mortality is to perform regular screening in search for polyps and colonoscopy is the screening tool of choice. The main limitations of this screening procedure are polyp miss-rate and inability to perform visual assessment of polyp malignancy. These drawbacks can be reduced by designing Decision Support Systems (DSS) aiming to help clinicians in the different stages of the procedure by providing endoluminal scene segmentation. Thus, in this paper, we introduce an extended benchmark of colonoscopy image, with the hope of establishing a new strong benchmark for colonoscopy image analysis research. We provide new baselines on this dataset by training standard fully convolutional networks (FCN) for semantic segmentation and significantly outperforming, without any further post-processing, prior results in endoluminal scene segmentation.</p>
                        </div>
                      </div>
                      <a class="ext_link" href="http://www.cvc.uab.es/CVC-Colon/"><i class="fa fa-external-link"></i></a>
                      <div style="clear:both"></div>
                    </div>
                  </figcaption> </figure>
                <!-- End Portfolio Item -->

                <!-- Portfolio Item -->
                <figure class="effect-milo">
                    <img src="images/works/pixelvae.jpg" alt="img11" height="222" width="282">
                    <figcaption> <span class="label">Image Generation</span>
                    <div class="portfolio_button">
                      <h3>PixelVAE: A Latent Variable Model for Natural Images</h3>
                      <a href=".work5" class="open_popup" data-effect="mfp-zoom-out"><i class="hovicon effect-9 sub-b"><i class="fa fa-search"></i></i></a>
                    </div>
                    <div class="mfp-hide mfp-with-anim work_desc work5">
                      <div class="col-md-6">
                        <div class="image_work">
                            <img src="images/works/pixelvae.jpg" alt="img" height="420" width="560">
                            <!-- <div class="video_container"> <iframe src="https://www.youtube.com/embed/RgAkfbA0sSY" frameborder="0" allowfullscreen class="video_iframe"></iframe> </div> -->
                        </div>
                      </div>
                      <div class="col-md-6">
                        <div class="project_content">
                          <h2 class="project_title">PixelVAE: A Latent Variable Model for Natural Images</h2>
                          <p class="project_desc">Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and generate samples that preserve global structure but tend to suffer from image blurriness. PixelCNNs model sharp contours and details very well, but lack an explicit latent representation and have difficulty modeling large-scale structure in a computationally efficient way. In this paper, we present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. The resulting architecture achieves state-of-the-art log-likelihood on binarized MNIST. We extend PixelVAE to a hierarchy of multiple latent variables at different scales; this hierarchical model achieves competitive likelihood on 64x64 ImageNet and generates high-quality samples on LSUN bedrooms.</p>
                        </div>
                      </div>
                      <a class="ext_link" href="https://github.com/igul222/PixelVAE"><i class="fa fa-external-link"></i></a>
                      <div style="clear:both"></div>
                    </div>
                  </figcaption> </figure>
                <!-- End Portfolio Item -->

                <!-- Portfolio Item -->
                <figure class="effect-milo">
                    <img src="images/projects/synthia.png" alt="img11" height="222" width="282">
                    <figcaption> <span class="label">SYNTHIA</span>
                    <div class="portfolio_button">
                      <h3> dataset</h3>
                      <a href=".work6" class="open_popup" data-effect="mfp-zoom-out"><i class="hovicon effect-9 sub-b"><i class="fa fa-search"></i></i></a>
                    </div>
                    <div class="mfp-hide mfp-with-anim work_desc work6">
                      <div class="col-md-6">
                        <div class="image_work">
                            <div class="video_container"> <iframe src="https://www.youtube.com/embed/dh8S9pYkDIM" frameborder="0" allowfullscreen class="video_iframe"></iframe> </div>
                        </div>
                      </div>
                      <div class="col-md-6">
                        <div class="project_content">
                          <h2 class="project_title">SYNTHIA dataset</h2>
                          <p class="project_desc">SYNTHIA has been one of our latest Projects, where we have created a driving simulator in order to teach driverless cars how to drive. The simulator has been licensed to various international companies. <br><br>
                          SYNTHIA allows to generate datasets with the purpose of aiding scene understanding problems in the context of driving scenarios. The datasets consists of photo-realistic images rendered from a virtual city and comes with precise pixel-level semantic class and instance annotations (currently, it is cityscapes compatible), depth and optical flow. The CVC team is constantly incrementing the functionalities of SYNTHIA for generating more types of ground truth. Different conditions can be forced such as illumination, weather, season, and camera locations (multi-camera calibrated settings are possible). The current version of SYNTHIA allows generating more than 10000 images per hour with such a ground truth, using a game-graded regular GPU.</p></p>
                        </div>
                      </div>
                      <a class="ext_link" href="http://www.synthia-dataset.net"><i class="fa fa-external-link"></i></a>
                      <div style="clear:both"></div>
                    </div>
                  </figcaption> </figure>
                <!-- End Portfolio Item -->

              <!-- End Portfolio Wrapper --> </div>
          </article>
          <!-- End Works Section -->
          <!-- Contact Section -->
          <article class="hs-content contact-section" id="section8"> <span class="sec-icon fa fa-paper-plane"></span>
            <div class="hs-inner"> <span class="before-title">.08</span>
              <h2>CONTACT</h2>
              <div class="contact_info">
                <h3>Get in touch</h3>
                <hr>
                <h5>I'm always open to new opportunities</h5>
                <h6>Simply use the form below to get in touch</h6>
                <hr> </div>
              <!-- Contact Form -->
              <fieldset id="contact_form">
                <div id="result"></div>
                <input name="name" id="name" placeholder="NAME" type="text"> <input
                  name="email" id="email" placeholder="EMAIL" type="email"> <textarea
name="message" id="message" placeholder="MESSAGE"></textarea> <span class="submit_btn"
                  id="submit_btn">SEND MESSAGE</span> </fieldset>
              <!-- End Contact Form --> </div>
          </article>
          <!-- End Contact Section --> </div>
        <!-- End hs-content-wrapper --> </div>
      <!-- End hs-content-scroller --> </div>
    <!-- End container -->
    <div id="my-panel"> </div>
    <!-- PLUGIN SCRIPTS -->
    <script type="text/javascript" src="js/jquery.min.js"></script>
    <script type="text/javascript" src="js/bootstrap.min.js"></script>
    <script type="text/javascript" src="js/default.js"></script>
    <script type="text/javascript" src="https://maps.googleapis.com/maps/api/js?sensor=false"></script>
    <script type="text/javascript" src="js/watch.js"></script>
    <script type="text/javascript" src="js/layout.js"></script>
    <script type="text/javascript" src="js/main.js"></script>
    <!-- END PLUGIN SCRIPTS -->
  </body>
</html>
